\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % Required for author affiliations
\usepackage{indentfirst} % Indent first paragraph of sections
\usepackage{amssymb} % For mathematical symbols
\usepackage{amsthm} % For theorem environments
\usepackage{amsmath} % For advanced math typesetting
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\usepackage{pgfplots} % For plots
\usetikzlibrary{arrows.meta,calc}
\pgfplotsset{compat=1.18} % Set compatibility level
\usepackage{tikz} % For drawing shapes
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem*{example}{Example}
\newtheorem{remark}{Remark}
\reversemarginpar
\begin{document}
%------- Title page   -----------
\title{MATH 223: Linear Algebra}
\author{William Homier}
\affil[1]{McGill University Physics, 3600 Rue University, MontrÃ©al, QC H3A 2T8, Canada}
\date{January \(5^{th}\), 2026}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\maketitle

%------- Abstract -----------
\noindent\rule{\textwidth}{0.4pt}
\thispagestyle{empty}
\begin{abstract}

\end{abstract}
\noindent\rule{\textwidth}{0.4pt}
\clearpage

%------- Table of Contents -----------
\thispagestyle{empty}
\hypersetup{
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\tableofcontents
\clearpage

%------- introduction -----------
\setcounter{page}{1}
\section{Introduction}

\section{Prerequisite knowledge}
\subsection{Notation}
\subsubsection{Sets}
Sets are a grouping of objects.
\begin{center}
\begin{tabular}{c|p{6cm}|p{5cm}}
Set & Meaning & Examples \\
\hline
$\mathbb{N}$ & The set of natural numbers & (0, 1, 2, 3, ...)\\
$\mathbb{Z}$ & The set of integers & (..., -3, -2, -1, 0, 1, 2, 3, ...)\\
$\mathbb{Q}$ & The set of rational numbers & \(\mathbb{Q} = {\frac{a}{b}\:|\:\forall a,b \in \mathbb{Z}\:and\:b \neq 0}\)\\
$\mathbb{R}$ & The set of all rational and all irrational numbers & \((...,-1,0,\frac{1}{4},1,1000,...)\)\\
$\mathbb{C}$ & The set of all complex numbers & \(\mathbb{C} = \{x + iy\:|\:x,y \in \mathbb{R}\:and\:i \subseteq \sqrt{-1}\}\).\\
\end{tabular}
\end{center}

We have the following relationships between sets:
\[\mathbb{N} \subseteq \mathbb{Z} \subseteq \mathbb{Q} \subseteq \mathbb{R} \subseteq \mathbb{C}\]
\subsubsection{Symbols}
\begin{center}
\begin{tabular}{c|l}
Symbol & Meaning \\
\hline
$\subseteq$ & is a subset of or equal to \\
$\subset$ & is a strict subset of \\
$\in$ & is an element of \\
$\forall$ & for all \\
$\exists$ & there exists \\
$\emptyset$ & empty set \\
$\Rightarrow$ & implies \\
$\Leftrightarrow$ & if and only if
\end{tabular}
\end{center}

\subsection{Complex Algebra}
\subsubsection{Complex Numbers}
A complex number is of the form: \(z = x + iy\) where \(x,y \in \mathbb{R}\) and \(i\) is the imaginary unit such that \(i^2 + 1 = 0\).
\begin{definition}[Powers of \(i\)]
\begin{tabular}{c|cccccc}
k & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
$i^k$ & 1 & i & -1 & -i & 1 & i
\end{tabular}
\end{definition}
\begin{theorem}[Fundamental Theorem of Algebra]
    Any polynomial\footnote{Polynomial is a function such as: \(f = a_n x^n + a_{n-1}x^{n-1} + ... + a_1 x + a_0\) where \(a_i \in \mathbb{R}\) or \(\mathbb{C}\) and \(n \in \mathbb{N}\).} \(f\) (except constant functions) has a root in \(\mathbb{C}\).
\end{theorem}
\begin{remark}
    If we have a polynomial \(f\) of degree \(n\), then it has \(n\) roots, where each root can have a multiplicity\footnote{The multiplicity of a root represents how many times the root occurs in the polynomial.}. For example, if we have a polynomial \((x-1)^2\), it has a degree of 2 but only one root, which is 1, with a multiplicity of 2. This means that the root 1 appears twice in the polynomial.
\end{remark}
We can factorize a polynomial in the form of \(f = a_n x^n + ... + a_1 x + a_0\) into a linear factor: \(f = a(x - z_1)(x - z_2)...(x - z_n)\) where \(z_i\) are the roots of \(f\) in \(\mathbb{C}\).\\\\
Using the FTA for a function such as \(f = a_nx^n + ... + a_1 x + a_0\), we can say that the FTA implies that \(f\) has a root \(f(z) = 0\).
\subsubsection{Complex Operations}
\label{subsec:CO}
We can define operations on complex numbers as follows:
\begin{itemize}
    \item Addition: \(z + z' = (x + x') + i(y + y')\), where \(x,x',y,y' \in \mathbb{R}\).
    \item Multiplication: \(zz' = (x + iy)(x' + iy') = (xx' - yy') + i(xy' + yx')\).
    \item Inverse: \(\frac{1}{z} = \frac{\overline{z}}{z\overline{z}} = \frac{x - iy}{x^2 + y^2} = \frac{x}{x^2 + y^2} + i\frac{-y}{x^2 + y^2}\)
\end{itemize}
From the definition of inverse, we can see that for any complex number \(z\), its inverse \(\frac{1}{z}\) is also a complex number. For example, take \(z = 1 + i\), where \(x = y = 1\), from the definition of inverse, we can conclude that:
\[\frac{1}{1 + i} \in \mathbb{C}\]
Multiplying by a complex number \(z\) corresponds geometrically to
\[\begin{cases}\text{a rotation by some angle } \theta,\\\text{a rescaling by the factor } |z|.\end{cases}\]
\subsubsection{Complex Conjugate}
A complex conjugate is a way to "flip" the imaginary part of a complex number. For example, if we have a complex number \(z = x + iy\), then the complex conjugate of \(z\) is \(\overline{z} = x - iy\). Some basic properties of complex conjugates are:
\begin{itemize}
    \item \(\overline{z} = z\)
    \item \(\overline{z + z'} = \overline{z} + \overline{z'}\)
    \item \(\overline{z \cdot z'} = \overline{z} \cdot \overline{z'}\)
\end{itemize}

\subsubsection{Geometric and Polar Form of Complex Numbers}
\marginpar{January 09, 2026.}

\paragraph{1. Geometric Interpretation}
\begin{definition}[Geometric interpretation]
    Every complex number \(z = x + iy\) can be identified with a point \((x,y)\) in the plane, called the complex plane.
\end{definition}
We define the complex plane as:
\[\mathbb{C} = \{x + iy \mid x,y \in \mathbb{R}\}.\]

\begin{center}
\begin{tikzpicture}[scale=2]
% Axes
\draw[->] (-1.4,0)--(1.4,0) node[right]{$x$};
\draw[->] (0,-1.4)--(0,1.4) node[above]{$y$};

% Unit circle
\draw (0,0) circle(1);

% Point z = x+iy
\coordinate (Z) at (0.8,0.6);
\draw[fill] (Z) circle(0.02) node[above right] {$z=x+iy$};

% Projections
\draw[dashed] (Z) -- (0.8,0) node[below] {$x$};
\draw[dashed] (Z) -- (0,0.6) node[left] {$y$};

% Horizontal from y-axis and vertical from x-axis
\draw (0,0.6) -- (Z);
\draw (0.8,0) -- (Z);

% Radius
\draw (0,0) -- (Z);

\end{tikzpicture}
\end{center}

\paragraph{2. Modulus and Unit Circle}
\begin{definition}[Modulus]
    The modulus of a complex number \(z\) is defined by
    \[|z| = \sqrt{z\overline{z}} = \sqrt{x^2 + y^2}.\]
    Geometrically, \(|z|\) is the distance from the origin to the point \((x,y)\).
\end{definition}
We can rewrite the definition of the unit circle as follows:
\[S' = \{(x,y) \in \mathbb{R}^2 : x^2 + y^2 = 1\} = \{ z \in \mathbb{C} : |z| = 1\},\]
where \(S'\) is the unit circle in the complex plane.

\paragraph{3. Polar Coordinates}

\begin{definition}[Polar coordinates]
    Instead of describing a point by \((x,y)\), we may describe it using polar coordinates \((r,\theta)\), where \(r = |z|\) is the distance to the origin and \(\theta\) is the angle with the positive \(x\)-axis.
\end{definition}
\begin{example}
    Consider the point \((x,y)\), where \(x = r \cos(\theta)\:and\:y = r \sin(\theta)\). We can define \((r,\theta)\) as follows:
    \begin{center}
    \begin{tikzpicture}[scale=2]
    % Axes
    \draw[->] (-1.4,0)--(1.4,0) node[right]{$x$};
    \draw[->] (0,-1.4)--(0,1.4) node[above]{$y$};

    % Point
    \coordinate (Z) at (0.9,0.5);
    \draw[fill] (Z) circle(0.02) node[above right] {$(x,y)$};

    % Lines
    \draw (0,0) -- (Z) node[midway, above] {$r$};
    \draw[dashed] (Z) -- (0.9,0);
    \draw[dashed] (Z) -- (0,0.5);

    % Angle theta
    \draw (0.4,0) arc (0:29:0.4);
    \node at (0.45,0.12) {$\theta$};

    \end{tikzpicture}
    \end{center}
\end{example}
Complex numbers can also be described using polar coordinates.
\begin{definition}[Polar and exponential form]
    \[z = x + iy = r \cos(\theta) + ir \sin(\theta) = r(\cos(\theta) + i \sin(\theta)) = re^{i\theta}.\]
\end{definition}
We can also define multiplication in polar form:
\begin{definition}[Multiplication in polar form]
    \[z = re^{i\theta}, \quad z' = r'e^{i\theta'}, \quad zz' = rr'e^{i(\theta + \theta')}.\]
\end{definition}
\begin{example}
    \[(1 + i)^{32} = (\sqrt{2}e^{i\pi/4})^{32} = (\sqrt{2})^{32} e^{i8\pi} = 2^{16}(\cos 8\pi + i \sin 8\pi) = 2^{16}.\]
\end{example}

Around the 1740, the mathematician Euler discovered a formula for complex numbers. The formula is known as Euler's formula.
\begin{definition}[Euler's formula]
    \[e^{i\theta} = \cos(\theta) + i \sin(\theta).\]
\end{definition}
This formula is quite useful when dealing with complex numbers, as it allows us to write complex numbers in a more compact form.

\paragraph{4. Roots}
\begin{definition}[\(n^{\text{th}}\) roots]
    An \(n^{\text{th}}\) root of \(z\) is a complex number \(w\) such that
    \[w^n = z.\]
\end{definition}
If \(z = re^{i\theta}\), then any solution of \(w^n = z\) must satisfy
\[w^n = r e^{i\theta}.\]
Writing \(w = \rho e^{i\varphi}\), we obtain
\[\rho^n = r, \quad n\varphi = \theta + 2\pi k, \quad k \in \mathbb{Z}.\]
Hence, all \(n^{\text{th}}\) roots of \(z\) are
\[w_k = r^{1/n} e^{i(\theta + 2\pi k)/n}, \quad k = 0,1,2,\dots,n-1.\]


\begin{definition}[Roots of unity]
The \(n^{\text{th}}\) roots of unity are the solutions of
\[w^n = (e^{i2\pi/n})^n = e^{i2\pi} = 1,\:w \in \mathbb{C}.\]
Since \(1 = e^{i2\pi k}\), they are given by
\[w_k = e^{i2\pi k/n},\: k = 0,1,2,\dots,n-1 \in \mathbb{Z}.\]
\end{definition}
Geometrically, the \(n^{\text{th}}\) roots of unity lie on the unit circle and are equally spaced.

\begin{center}
\begin{tikzpicture}[scale=2]
% Axes
\draw[->] (-1.3,0)--(1.3,0);
\draw[->] (0,-1.3)--(0,1.3);

% Unit circle
\draw (0,0) circle(1);

% Example: n = 6 roots
\foreach \k in {0,1,2,3,4,5}{
\coordinate (P\k) at ({cos(60*\k)},{sin(60*\k)});
\draw[fill] (P\k) circle(0.02);
\draw (0,0) -- (P\k);
}

\node at (0.75,0.75) {$|w| = 1$};
\end{tikzpicture}
\end{center}

\section{Basic Algebraic structures}
\subsection{Sets with Multiplication}
\begin{definition}[Set with multiplication]
A set with multiplication is a set $M$ where you can multiply any two elements of $M$, and the result is still an element of $M$. Formally, this means there is a rule $\cdot$ that takes any pair $(a, a)$ from $M$ and produces an element $ab$ that is also in $M$:
\[\cdot : M \times M \to M\]
\end{definition}

\subsection{Invertibility}
\begin{definition}[Condition for Invertibility]
    Let \(A \in M\) be an \(n \times n\) matrix, and suppose that there exists an \(n \times n\) matrix \(B\) such that \(AB = I_n\) or \(BA = I_n\). Where \(I_n\) is the \(n \times n\) identity matrix\footnote{An identity matrix is a square matrix with 1s on its main diagonal and 0s everywhere else. It represents no change in linear transformations, and it's used in finding matrix inverses.}$\begin{bmatrix}1&0&0\\0&\ldots&0\\0&0&1\end{bmatrix}$. Then A is invertible, and \(B\) is called the inverse of \(A\) and is denoted by \(B = A^{-1}\).
\end{definition}

\begin{remark}
    If \(A\) is invertible, then \(A^{-1}\) exists and is unique\footnote{Unique means there is exactly one such element.}.
\end{remark}

To determine if an element \(A\) in a set with multiplication \(M\) is invertible, we can use the following examples:
\begin{example}
    Let \(M = \mathbb{Z} = \{...-2,-1,0,1,2,...\}\) and \(A = 2\). Is \(A\) invertible in \(M\)?\\
    Solution: No, because \(\frac{1}{2} \notin \mathbb{Z}\).
\end{example}
\begin{example}
    Let \(M = \mathbb{R}\) and \(A = 2\), is \(A\) invertible in \(M\)?\\
    Solution: Yes, because \(\frac{1}{2} \in \mathbb{R}\).
\end{example}
\begin{example}
    Is \(1 + i\) invertible in \(\mathbb{C}\)?\\
    Solution: Yes, using our previous definition of inverse (\ref{subsec:CO}), we get that \[\frac{1}{1 + i} = \frac{1 - i}{2} \in \mathbb{C}.\]
\end{example}

\subsection{Ring}
\begin{definition}
A \textbf{ring} is a set \(\mathbb{R}\) where you can \textbf{add} and \textbf{multiply} elements, and the following are true:

\begin{enumerate}
    \item You can add any two elements and stay in \(\mathbb{R}\). There is a zero, negatives exist, and the order of addition does not matter.
    \item You can multiply any two elements and stay in \(\mathbb{R}\). There is a \(1\), and multiplication is associative.
    \item Multiplication distributes over addition:
    \[
    a(b+c) = ab + ac \quad \text{and} \quad (a+b)c = ac + bc.
    \]
\end{enumerate}
\end{definition}
The main example of a ring is the set of integers \(\mathbb{Z}\).
\subsection{Field}
\begin{definition}
A field is a commutative ring in which every element is invertible.
\end{definition}
The main example of a field is the real numbers \(\mathbb{R}\) or the complex numbers \(\mathbb{C}\). Mathematically, \(K = \mathbb{R}\) or \(\mathbb{C}\), where \(K\) is the field.\\
In the following explanation, we will denote \(M\) to be a set with multiplication.
An example of a set with multiplication is the set of all \(2 \times 2\) complex matrices: \(M = M_2(\mathbb{C})\). Another example is the nonzero set of all real numbers \(\mathbb{R}\) with ordinary multiplication: \(\mathbb{R}^* = \mathbb{R} \setminus \{0\}\).

\begin{problem}
    Construct a field with 2 elements.
\end{problem}
\begin{problem}
    Show that if an inverse of A in \(\mathbb{M}\) exists, then it is unique.
\end{problem}
\begin{problem}
    Let \(K\) be a field. Prove that this matrix $\begin{bmatrix}0&1\\0&0\end{bmatrix}$ \(\in M_2(K)\) is not invertible.
\end{problem}

\section{Vector Spaces}
\marginpar{January 09, 2026.}
\subsection{Cartesian vector spaces}
\begin{definition}[$R^n$]
Let \(n \in \mathbb{N}\). The Cartesian product of \(n\) copies of \(\mathbb{R}\) is called \(\mathbb{R}^n\).

\[\mathbb{R}^n = \begin{cases}
    \begin{pmatrix}
        x_1\\
        \ldots\\
        x_n
    \end{pmatrix} : x_1, \ldots, x_n \in \mathbb{R}
\end{cases}
\]
\end{definition}

\subsection{Vectors}
\subsubsection{Vector operations}
Vector operations are defined as follows.\\
Addition: \(\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} + \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} = \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}\),\\
scalar multiplication: \(\lambda \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} = \begin{pmatrix} \lambda x_1 \\ \vdots \\ \lambda x_n \end{pmatrix}\), where \(\lambda \in \mathbb{R}\).

\begin{definition}[Linear combination]
    A linear combination of vectors \(v_1, \ldots, v_n\) is a vector \(v\) of the form \(v = \lambda_1v_1 + \ldots + \lambda_nv_n\), where \(\lambda_1, \ldots, \lambda_n \in \mathbb{R}\).
\end{definition}
\begin{example}
    A linear combination could look like this:
    \[\xi((v_1 + 2v_2) + v_3) + v_4,\]
    where $\xi \in \mathbb{R}$.
\end{example}

\subsubsection{Span}
\marginpar{January 12, 2026.}
\begin{definition}[Span]
Let \(A \subset \mathbb{R}^2\). The span of \(A\), denoted \(Span(A)\), is the set of all linear combinations of elements of \(A\).
\end{definition}
\begin{remark}
If \(A = \{v\}\) contains one nonzero vector, then \(\operatorname{Span}(v)\) is a line through the origin.
\end{remark}
\begin{example}
Let \(A = \begin{pmatrix}1\\1\end{pmatrix}\). Then
\[
\operatorname{Span}(A) = \left\{ t\begin{pmatrix}1\\1\end{pmatrix} \mid t \in \mathbb{R} \right\},
\]
which is a line in \(\mathbb{R}^2\).
\begin{center}
\begin{tikzpicture}[scale=1.1, >=Stealth]
\draw[->] (-3,0) -- (3,0);
\draw[->] (0,-3) -- (0,3);

\draw[thick] (-2.5,-2.5) -- (2.5,2.5);

\draw[->, thick] (0,0) -- (0.8,0.8);
\draw[->, thick] (0,0) -- (1.5,1.5);
\draw[->, thick] (0,0) -- (2.2,2.2);
\end{tikzpicture}
\end{center}
\end{example}

\paragraph{1. Span in $\mathbb{R}^n$}

\begin{example}
    The span of n vectors \(v_1,\ldots,v_n\) is the set of all linear combinations of \(v_1,\ldots,v_n\). Then any vector in \(\operatorname{Span}(v_1,\ldots,v_n)\) has the form
    \[Span(v_1,\ldots,v_n) = \{\lambda_2v_1+\ldots+\lambda_nv_n : \lambda_i \in \mathbb{R}\}\]
\end{example}
When working in \(\mathbb{R}^n\), the span describes all points you can reach by scaling and adding the given vectors.  
Depending on the vectors, the span can be a line (if the vectors are dependent), a plane, or a higher-dimensional subspace.  
The following examples show what spans look like in \(\mathbb{R}^2\).
\begin{example}
    Let \(v_1 = \begin{pmatrix}3\\1\end{pmatrix}\), \(v_2 = \begin{pmatrix}1\\3\end{pmatrix}\).
    Then \(\operatorname{Span}(v_1,v_2) = \{\lambda_1v_1+\lambda_2v_2 : \lambda_1,\lambda_2 \in \mathbb{R}\}\)
\end{example}
\begin{example}
    Let \(A = \begin{pmatrix}1\\1\end{pmatrix}\). Then \(\operatorname{Span}(A) = \{\lambda\begin{pmatrix}1\\1\end{pmatrix} : \lambda \in \mathbb{R}\}\)
    \begin{center}
        \begin{tikzpicture}[scale=1.1, >=Stealth]
        \draw[->] (-3,0) -- (3,0);
        \draw[->] (0,-3) -- (0,3);

        \draw[thick] (-2.5,-2.5) -- (2.5,2.5);

        \draw[->, thick] (0,0) -- (0.8,0.8);
        \draw[->, thick] (0,0) -- (1.5,1.5);
        \draw[->, thick] (0,0) -- (2.2,2.2);

        \node[right] at (2.5,2.5) {$\text{Span}\!\begin{pmatrix}1\\1\end{pmatrix}$};
        \end{tikzpicture}
    \end{center}
\end{example}
\begin{example}
Let
\[v_1=\begin{pmatrix}3\\1\end{pmatrix},\quad v_2=\begin{pmatrix}1\\3\end{pmatrix}.\]
Any vector in \(\operatorname{Span}(v_1,v_2)\) has the form
\(\lambda_1 v_1+\lambda_2 v_2\).
Geometrically, this is illustrated below.
\begin{center}
\begin{tikzpicture}[scale=1, >=Stealth]
\draw[->] (-0.5,0) -- (5,0);
\draw[->] (0,-0.5) -- (0,4);

\coordinate (v1) at (3,1);
\coordinate (v2) at (1,3);

\draw[->, thick] (0,0) -- (v1) node[right] {$\lambda_1 v_1$};
\draw[->, thick] (0,0) -- (v2) node[left] {$\lambda_2 v_2$};

\draw[dashed] (v1) -- ($(v1)+(v2)$);
\draw[dashed] (v2) -- ($(v1)+(v2)$);

\draw[->, thick] (0,0) -- ($(v1)+(v2)$) node[above right] {$\lambda_1 v_1+\lambda_2 v_2$};

\node at (3.2,3.2) {$\in \operatorname{Span}(v_1,v_2)$};
\end{tikzpicture}
\end{center}
\end{example}

\begin{example}
Let 
\[
v_1=\begin{pmatrix}2\\1\end{pmatrix}, 
\quad 
v_2=\begin{pmatrix}4\\2\end{pmatrix}.
\]
Then \(v_2 = 2v_1\), so
\[
\operatorname{Span}(v_1)=\operatorname{Span}(v_2).
\]
\begin{center}
\begin{tikzpicture}[scale=1.1, >=Stealth]
\draw[->] (-3,0) -- (3,0);
\draw[->] (0,-3) -- (0,3);

\draw[thick] (-2.5,-1.25) -- (2.5,1.25);

\draw[->, thick] (0,0) -- (1,0.5) node[right] {$v_1$};
\draw[->, thick] (0,0) -- (2,1) node[right] {$v_2$};

\node at (1.8,-1.6) {$\operatorname{Span}(v_1)=\operatorname{Span}(v_2)$};
\end{tikzpicture}
\end{center}
\end{example}

\paragraph{Span in $\mathbb{C}^n$}
\begin{definition}
The span over \(\mathbb{C}\) of vectors \(v_1,\ldots,v_n\) is
\[Span_{\mathbb{C}}(v_1,\ldots,v_n)=\{\lambda_1 v_1 + \ldots + \lambda_n v_n : \lambda_i \in \mathbb{C}\}.\]
\end{definition}
\begin{example}
For example,
\[\begin{pmatrix}2i\\2\end{pmatrix} + i\begin{pmatrix}3\\1+i\end{pmatrix} \in \mathbb{C}^2.\]
\end{example}

\medskip
\noindent
So we distinguish between real and complex vector spaces:
\[
\mathbb{R}^n = \left\{\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} : x_i \in \mathbb{R}\right\},
\qquad
\mathbb{C}^n = \left\{\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} : x_i \in \mathbb{C}\right\}.
\]

\subsubsection{Standard Basis}

\begin{definition}
The standard basis for \(\mathbb{R}^n\) is the set \(\{e_1,\ldots,e_n\}\), where \(e_i\) is the \(i\)th standard basis vector.

\[\mathbb{R}^n = \left\{\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} : x_i \in \mathbb{R}\right\}\]

\[e_1 = \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}, \quad \ldots \quad e_n = \begin{pmatrix}0\\\vdots\\0\\1\end{pmatrix}\]
\end{definition}

\medskip
\noindent
(Basis = span + linear independence)

\paragraph{Claim}
These vectors span \(\mathbb{R}^n\).

\begin{proof}
We show that any vector in \(\mathbb{R}^n\) can be written as a linear combination of \(e_1,\ldots,e_n\):
\[\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} = \begin{pmatrix}x_1\\0\\\vdots\\0\end{pmatrix} + \begin{pmatrix}0\\x_2\\\vdots\\0\end{pmatrix} + \cdots + \begin{pmatrix}0\\\vdots\\0\\x_n\end{pmatrix} = x_1 e_1 + x_2 e_2 + \cdots + x_n e_n.\]
\end{proof}

\begin{example}[Complex vs Real Span]
    \[\mathbb{C}^2 = \operatorname{Span}_{\mathbb{C}}(e_1,e_2)
    = \{z_1 e_1 + z_2 e_2 : z_1,z_2 \in \mathbb{C}\},\]
    where 
    \[e_1=\begin{pmatrix}1\\0\end{pmatrix}, \quad e_2=\begin{pmatrix}0\\1\end{pmatrix}.\]
    Here \(\dim_{\mathbb{C}}\mathbb{C}^2=2\) but \(\dim_{\mathbb{R}}\mathbb{C}^2=4\).
    Every complex scalar can be written as \(z_k=x_k+iy_k\) with \(x_k,y_k \in \mathbb{R}\). So
    \[\operatorname{Span}_{\mathbb{C}}(e_1,e_2) = \{(x_1+iy_1)e_1 + (x_2+iy_2)e_2 : x_1,x_2,y_1,y_2 \in \mathbb{R}\}.\]
    Separating real and imaginary parts,
    \[\operatorname{Span}_{\mathbb{C}}(e_1,e_2) = x_1 e_1 + x_2 e_2 + y_1 (i e_1) + y_2 (i e_2).\]
    Therefore,
    \[\operatorname{Span}_{\mathbb{C}}(e_1,e_2) = \operatorname{Span}_{\mathbb{R}}\left(
    \begin{pmatrix}1\\0\end{pmatrix},
    \begin{pmatrix}0\\1\end{pmatrix},
    \begin{pmatrix}i\\0\end{pmatrix},
    \begin{pmatrix}0\\i\end{pmatrix}
    \right).
    \]

    \begin{center}
    \begin{tikzpicture}[scale=1.2, >=Stealth]
    \draw[->] (-3,0) -- (3,0) node[right] {$\Re$};
    \draw[->] (0,-3) -- (0,3) node[above] {$\Im$};

    \draw[->, thick] (0,0) -- (2,0) node[below] {$1$};
    \draw[->, thick] (0,0) -- (0,2) node[left] {$i$};

    \node at (1.5,1.5) {$\mathbb{C}=\text{span}_{\mathbb{R}}(1,i)$};
    \end{tikzpicture}
    \end{center}
\end{example}


\subsubsection{Abstract Vector Spaces}
An abstract vector space is just a set with two operations:
vector addition \((v_1+v_2)\) and scalar multiplication \((\lambda v)\), where \(\lambda\) comes from a field \(k\), satisfying certain axioms.

\begin{definition}
    Let \(k\) be a field. A \textbf{vector space over \(k\)} is a set \(V\) together with two operations
    \[
    \begin{cases}
    v_1+v_2 \in V, \quad v_1,v_2 \in V,\\
    \lambda v \in V, \quad v \in V, \ \lambda \in k,
    \end{cases}
    \]
    satisfying 8 axioms.
\end{definition}

\paragraph{Axioms}
(1) Rules for addition
\[(v_1+v_2)+v_3 = v_1+(v_2+v_3)\]
There exists a zero vector \(0 \in V\) such that \(0+v=v\).
For every \(v \in V\), there exists \(-v \in V\).
\[u+v = v+u\]
(2) Rules for scalar multiplication
\[1\cdot v = v\]
\[\lambda_1(\lambda_2 v) = (\lambda_1\lambda_2)v\]
(3) Compatibility (distributive laws)
\[(\lambda_1+\lambda_2)v = \lambda_1 v + \lambda_2 v\]
\[\lambda(v_1+v_2) = \lambda v_1 + \lambda v_2\]

\begin{example}
Let \(K\) be a field. Then
\[K^n = \left\{\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} : x_i \in K\right\}\]
is a vector space over \(K\).
\end{example}

\begin{remark}
\[\operatorname{Span}_{\mathbb{Z}}(e_1,e_2) = \{\lambda_1 e_1 + \lambda_2 e_2 : \lambda_i \in \mathbb{Z}\}.\]
\end{remark}

\begin{center}
\begin{tikzpicture}[scale=1.2, >=Stealth]
\draw[->] (-0.5,0) -- (3,0);
\draw[->] (0,-0.5) -- (0,3);

\draw[->, thick] (0,0) -- (2,0) node[below] {$e_1$};
\draw[->, thick] (0,0) -- (0,2) node[left] {$e_2$};

\node at (1.4,1.4) {$\mathbb{R}^2$};
\end{tikzpicture}
\end{center}

\subsubsection{Examples of Vector Spaces}
\marginpar{14 January, 2026}

We now list important examples of vector spaces.

\paragraph{1. Coordinate spaces}
Let \(k\) be a field (\(k=\mathbb{R}\) or \(k=\mathbb{C}\)).
\[
k^n = \left\{\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} : x_i \in k\right\}
\]
is a vector space over \(k\).

The standard basis is
\[
e_1=\begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}, \ \ldots \ ,
e_n=\begin{pmatrix}0\\\vdots\\0\\1\end{pmatrix}.
\]

\paragraph{2. Polynomial spaces}
A polynomial is an expression
\[
f(x)=a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0,
\quad a_i \in k.
\]

\[
P_n(k)=\{a_n x^n + \cdots + a_1 x + a_0 : a_i \in k\}
\]
is a vector space over \(k\).

Examples:
\[
1+x^2 \in P_2(\mathbb{R}), \qquad 1+i x^3 \in P_3(\mathbb{C}).
\]

The subscript must satisfy \(\deg(f)\le n\).

All polynomials:
\[
P_\infty = \{a_n x^n + \cdots + a_1 x + a_0 : a_i \in k, \ n\ge0\}.
\]

\[
P_0 \subseteq P_1 \subseteq P_2 \subseteq \cdots \subseteq P_\infty.
\]

A standard basis is \(\{1,x,x^2,\ldots,x^n\}\).  
Every \(f \in P_n\) can be written
\[
f=\lambda_0 + \lambda_1 x + \lambda_2 x^2 + \cdots + \lambda_n x^n.
\]

\paragraph{3. Matrix spaces}
\[
M_n(k)=\left\{\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{pmatrix}: a_{ij}\in k\right\}
\]
is a vector space over \(k\).

Standard basis matrices:
\[
e_{11}=\begin{pmatrix}1&0&\cdots&0\\0&0&\cdots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&0\end{pmatrix},\quad
e_{12}=\begin{pmatrix}0&1&\cdots&0\\0&0&\cdots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&0\end{pmatrix}, \ \ldots
\]

\[
\dim M_n(k)=n^2.
\]

Example in \(M_2(\mathbb{R})\):
\[
\begin{pmatrix}1&2\\3&4\end{pmatrix}
=1e_{11}+2e_{12}+3e_{21}+4e_{22}.
\]

\paragraph{4. Function spaces}
Let \(D\) be a set and \(k\) a field.
\[
F(D,k)=\{f:D\to k\}
\]
is a vector space over \(k\).

When \(D=\mathbb{R}\) and \(k=\mathbb{R}\), functions are drawn with horizontal axis \(D\) and vertical axis \(k\).

\section{Appendix}

\section{Solutions}
\begin{solution}
    A field with 2 elements can be constructed as follows:
    Let \(F = \{0, 1\}\) be a set with two elements. We define addition and multiplication operations on \(F\) as follows:
    \begin{itemize}
        \item \(0 + 0 = 0\)
        \item \(0 + 1 = 1\)
        \item \(1 + 0 = 1\)
        \item \(1 + 1 = 0\)
        \item \(0 \times 0 = 0\)
        \item \(0 \times 1 = 0\)
        \item \(1 \times 0 = 0\)
        \item \(1 \times 1 = 1\)
    \end{itemize}
\end{solution}
\begin{solution}
    Suppose \(B\) and \(B'\) are both inverses of \(A\). Then
    \[B = B I = B(AB') = (BA)B' = I B' = B'.\]
    Therefore, \(B = B'\), so the inverse is unique.
\end{solution}
\begin{solution}
    We can answer this problem with proof by contradiction. Let's
    suppose this matrix is invertible. By definition there exists B = $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ such that $\begin{bmatrix}0&1\\0&0\end{bmatrix}$ $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ = $\begin{bmatrix}1&0\\0&1\end{bmatrix}$.  We can rewrite this equation into: $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ = $\begin{bmatrix}0&1\\0&0\end{bmatrix}^{-1}$. The inverse of our matrix can be rewritten as $\frac{1}{0*0 - 1*0}\begin{bmatrix}0&-1\\0&0\end{bmatrix}$\footnote{Recall that an inverse of a $2 \times 2$ matrix is equal to its determinant multiplied with its conjugate}. But this is undefined since division by 0 is undefined. Therefore, our initial assumption that the matrix is invertible is false, and thus the matrix is not invertible.
\end{solution}

\section{Useful Links}

\end{document}