\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % Required for author affiliations
\usepackage{indentfirst} % Indent first paragraph of sections
\usepackage{amssymb} % For mathematical symbols
\usepackage{amsthm} % For theorem environments
\usepackage{amsmath} % For advanced math typesetting
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{pgfplots} % For plots
\usepackage{tikz} % For drawing shapes
\pgfplotsset{compat=1.18} % Set compatibility level
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem*{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\reversemarginpar
\hypersetup{
    colorlinks=true,
}
\begin{document}
%------- Title page   -----------
\title{MATH 325: Honours Ordinary Differential Equations}
\author{William Homier}
\affil[1]{McGill University Physics, 3600 Rue University, Montréal, QC H3A 2T8, Canada}
\date{January \(6^{th}\), 2026}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\maketitle

%------- Abstract -----------
\noindent\rule{\textwidth}{0.4pt}
\thispagestyle{empty}
\begin{abstract}

\end{abstract}
\noindent\rule{\textwidth}{0.4pt}
\clearpage

%------- Table of Contents -----------
\thispagestyle{empty}
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\clearpage

%------- introduction -----------
\setcounter{page}{1}
\section{Introduction}
Jean-Philippe Lessard (Burnside 1119).
Tutorials every wednesday from 9am to 10am, ENGTR 0070, with Eunpyo Bang.
Office hours thursday.
No textbooks.
25\% assignments (2 written assignments 15\%, and 5 webworks 10\%).
25\% Midterm (February 16 - inclass).
50\% Final.
Since its honours you will deal with analysis.

\section{Prerequisite knowledge}
\subsection{Analysis}

\section{Intro, Classification, Theorem of Existence \& Uniqueness}
\subsection{Intro}
\marginpar{January 06, 2026.}
\begin{definition}[Differential Equation]
    A differential equation (DE) is a relation that involves an unknown function and some of its derivatives.
\end{definition}
\begin{example}[Motion under gravity and linear drag]
    Imagine a ball of mass \(m\) falling, subject to gravity and air resistance (drag). Denote by \(v(t)\) the velocity of the ball at time \(t\). Let the downward direction be positive. We know the force of gravity is given by \(F_g = mg\), where \(g\) is the acceleration due to gravity. The drag force is given by \(F_d = -\lambda v\), where \(\lambda\) is the drag coefficient and \(\lambda \geq 0\). According to Newton's second law\footnote{\(\sum_{i} F_i = ma\)}, the net force acting on the ball is equal to its mass times its acceleration
    \[m\frac{dv}{dt} = mg - \lambda v.\]
    Let \(y(t)\) be the position, meaning \(v(t) = \frac{dy}{dt}\). Then, we can rewrite the above equation as
    \[my'' + \lambda y' = mg.\]
\end{example}
\begin{example}[The Malthusian Growth Model]
    Denote by \(N(t)\) the size of a given population at time t. In an "unconstraint" environment, it is reasonable to assume that the rate of change of the number of individuals is proportional to the number of individuals present. This assumption leads to the following differential equation:
    \[\frac{dN}{dt} = rN,\]
    where \(r\) is called the growth rate (if \(r > 0\)), and decay rate (if \(r < 0\)). Assume that \(N > 0\). Using the chain rule and assuming that \(N(t)\) satisfies \(N' = rN\)
    \[\frac{dN}{N} = rdt,\]
    integrate both sides
    \[ln(N(t)) = rt + C,\]
    where \(C\) is the constant of integration. Exponentiating both sides, we obtain
    \[N(t) = e^Ce^{rt} = ke^{rt},\]
    where \(\{k > 0 | k \in \mathbb{R}\}\) is the initial population size at time \(t = 0\).\\
    Assume that an initial population (condition) is given:
    \[N(0) = N_0 (fixed),\]
    we therefore get that \(k = N_0\), and the unique solution that satisfies the initial condition is
    \[N(t) = N_0e^{rt}.\]
\end{example}
\begin{example}[The Logistic Equation]
    Now assume that our growth rate depends on the population size \(N(t)\) itself, therefore we get that
    \[\frac{dN}{dt} = R(N)N.\]
    Denote by K the number of individual that the environment can carry. K is called the carrying capacity\footnote{maximum population size that the environment can sustain indefinitely.} of the environment. If \(N < K\), we want growth \((R(N) > 0)\) and if \(N > K\), we want decay \((R(N) < 0)\).
    Let's pick the simplest function \(R(N)\) that satisfies \(R(0) = r\), \(R(K) = 0\) and is linear. We get that
    \[R(N) = r(1 - \frac{N}{K}).\]
    Therefore, our differential equation becomes
    \[\frac{dN}{dt} = \frac{r}{K}(K - N(t))N(t).\]
    This is called the logistic equation.
\end{example}
\begin{definition}[Ordinary Differential Equation]
    An ordinary differential equation (ODE) is a differential equation whose unknown function depends on one independent variable only.
\end{definition}
\begin{example} Examples of ODEs
    \begin{itemize}
        \item \(y''(t) + y'(t) + 2y(t) = sin(t)\)
        \item \(N'(t) = rN(t)\)
        \item \(mv'(t) = mg - \lambda v(t)\)
        \item \(y'(x) + 3y(x) = e^x\)
    \end{itemize}
\end{example}
\begin{definition}[Partial Differential Equation]
    A partial differential equation (PDE) is a differential equation whose unknown function depends on more than one independent variable. \textbf{Will not be taught in this course.}
\end{definition}
\begin{example}[Heat Equation]
    The Heat Equation is an example of a PDE. Let \(u = u(x,t)\), \(\frac{\partial u}{\partial t} = \frac{\partial^2 u}{\partial x^2}\). This PDE denotes the temperature of a body at time t and at position x.
\end{example}

\subsection{Classification}
\marginpar{January 08, 2026.}
\subsubsection{The Order}
\begin{definition}
    The order of an ODE is the order of the highest derivative that appears in the equation.
\end{definition}
\begin{example}
    \(N' = rN\) (first order ODE)
\end{example}
\begin{example}
    \(y''(t) + 2y'(t) = e^t\) (second order ODE)
\end{example}

\subsubsection{Dimension of the State}
\begin{definition}[Scalar ODE]
    A scalar ODE has one unknown function.
\end{definition}
\begin{example}[Scalar ODE]
    \[y'' + y = 0\]
\end{example}
\begin{definition}[System of ODEs]
    A system of ODEs has several unknown functions.
\end{definition}
\begin{example}[System of ODEs]
    \[\begin{cases}
        y_1' = y_2\\
        y_2' = -y_1
    \end{cases}\]
\end{example}
\begin{definition}[System of first order ODEs]
    A first order system has the form
    \[y'(t)=f(y(t),t),\]
    where \(y(t)\in\mathbb{R}^n\) and \(f:\mathbb{R}^n\times\mathbb{R}\to\mathbb{R}^n\).
    In components,
    \[y_i'(t)=f_i(y_1,\dots,y_n,t), \quad i=1,\dots,n.\]
    Any \(n\)th-order ODE
    \[y^{(n)}=G(t,y,y',\dots,y^{(n-1)})\]
    can be written as a first order system by setting
    \[y_1=y,\; y_2=y',\; \dots,\; y_n=y^{(n-1)},\]
    which gives
    \[\begin{cases}y_1'=y_2,\\y_2'=y_3,\\\vdots\\y_{n-1}'=y_n,\\y_n'=G(t,y_1,\dots,y_n).\end{cases}\]
\end{definition}
\begin{example}[System of First Order ODEs]
    Consider the second-order ODE
    \[y'' + 2y' + y = e^t.\]
    Define new variables
    \[y_1 = y, \qquad y_2 = y'.\]
    Then the system becomes
    \[\begin{pmatrix}y_1' \\ y_2'\end{pmatrix} = \begin{pmatrix}0 & 1 \\ -1 & -2\end{pmatrix} \begin{pmatrix}y_1 \\ y_2\end{pmatrix} + \begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]
    Let
    \[A = \begin{pmatrix} 0 & 1 \\ -1 & -2 \end{pmatrix},\quad r(t) = \begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]
\end{example}
\begin{problem}[System of First Order ODEs]
    Rewrite this third order ODE as a first order system:
    \[y''' + 4y' - y = 0\]
\end{problem}

\subsubsection{Linearity}
\begin{definition}[Linearity]
    A linear Ordinary Differential Equation (ODE) is one where the unknown function (\(y\)) and its derivatives (\(y^{\prime },y^{\prime \prime },\dots \)) appear only to the first power, are not multiplied together, and are not part of special functions like \(\sin (y)\) or \(e^{y}\). Essentially, they are "simple" combinations (addition/subtraction) of \(y\) and its derivatives, potentially multiplied by functions of the independent variable (like \(x\) or \(t\)).
\end{definition}
\begin{example}[Linearity]
    Consider the following ODEs:
    \begin{itemize}
        \item Linear: $y' +3y = 0$
        \item Linear: $y'' - 2xy' + y = cos(x)$
        \item Non-linear: $y' + y^2 = 0$
        \item Non-linear: $y'' + sin(y) = 0$
    \end{itemize}
\end{example}
\begin{example}[Lorenz system]
    \[\begin{aligned}y_1' &= \sigma (y_2-y_1), \\ y_2' &= \rho y_1 - y_2 - y_1y_3, \\ y_3' &= y_1y_2 - \beta y_3,\end{aligned}\]
    where \(\sigma,\rho,\beta\) are parameters. This is a nonlinear first order system in \(\mathbb{R}^3\).
\end{example}

\subsubsection{Autonomy}
\begin{definition}[Autonomy]
    The \(n\)\textsuperscript{th} order ODE \(F(t, y, y', \ldots, y^{(n)}) = 0\) is autonomous if \(F\) does not depend explicitly on t, that is, it can be written as \(F(y, y', \ldots, y^{(n)}) = 0\). Otherwise, it is non-autonomous.
\end{definition}
\begin{example}[Autonomy]
    Consider the following ODEs:
    \begin{itemize}
        \item Non-autonomous: \(y'' + 2y' + y - e^t = 0\)
        \item Autonomous: \(N'(t) = rN(t)\)
        \item Non-autonomous: \(y'(t) = ty(t)\)
    \end{itemize}
\end{example}
\begin{example}
    A classic, simple example of an autonomous first-order system is the linear growth/decay model:
    \[\frac{dy}{dt} = ky\]
    Here, \(f(y,t) = ky\), which depends only on \(y\) (where \(k\) is a constant), making it autonomous.
\end{example}
\begin{remark}
    The Lorenz system is an example of an autonomous system.
\end{remark} 

\subsubsection{Solutions of ODEs}
\begin{definition}[Solution of a first order system]
Let \(f: D\times I \to \mathbb{R}^n\).\footnote{\(f\) takes a state \(y\in D \subset \mathbb{R}^n\) and a time \(t\in I \subset \mathbb{R}\), and outputs a vector in \(\mathbb{R}^n\).}
A solution of $y'(t)=f(y(t),t)$ on an interval \(J\subset I\) is a differentiable function \(y:J\to D\)(\footnote{\(y(t)\) returns a vector of \(n\) components, so \(y(t)\in\mathbb{R}^n\). The set \(D\) is simply where these values are allowed to live.}) such that
\[y'(t)=f(y(t),t)\quad \text{for all } t\in J.\]
\end{definition}
\paragraph{1. Explicit Solutions}
\begin{example}[Explicit Solution]
    Consider the following ODE
    \[y' + y = 1.\]
    We can verify that \(y(t) = e^{-t} + 1\), and therefore \(y'(t) = -e^{-t}\), is a solution on \(\mathbb{R}\). Indeed,
    \[y' + y = -e^{-t} + (e^{-t} + 1) = 1.\]
\end{example}
\paragraph{2. Implicit solutions}
\begin{example}[Implicit Solution]
    Consider the ODE
    \[y \frac{dy}{dx} = x.\]
    This is a nonautonomous, nonlinear, first-order scalar ODE. Separating variables gives
    \[ydy = xdx.\]
    Integrating,
    \[\frac{1}{2}y^2 = \frac{1}{2}x^2 + C,\]
    or equivalently, the implicit solution
    \[x^2 - y^2 = C, \quad C \in \mathbb{R}.\]
\end{example}

\subsection{Initial Value Problems}
\marginpar{January 13, 2026}
\begin{definition}[Initial Value Problem (IVP)]
    An initial value problem consists of an ODE together with a condition $y(t_0)=y_0$, where the value of the solution is prescribed at an initial time \(t_0\).
\end{definition}
\begin{example}[IVP]
    Consider the following ODE 
    \[y' = f(y,t), \quad f : D \times (a,b) \to \mathbb{R}^n.\]
    Let \(t_0 \in (a,b)\). An initial condition is
    \[y(t_0) = y_0 \in \mathbb{R}^n.\]
    An initial value problem (IVP) is
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}.\]
\end{example}

\subsection{Existence and Uniqueness Theorem}
\subsubsection{Lipschitz continuity}
\begin{definition}[Lipschitz continuity]
    Let \(D \subseteq \mathbb{R}^n\) and let \(||\cdot||\) be a norm on \(\mathbb{R}^n\).
    A function \(f : D \to \mathbb{R}^n\) is Lipschitz continuous if there exists \(L \ge 0\) such that
    \[z||f(y_1) - f(y_2)|| \le L||y_1 - y_2|| \quad \forall y_1,y_2 \in D, z \in \mathbb{R}.\]
    The smallest such \(L\) is called the Lipschitz constant and is denoted \(Lip(f)\).
\end{definition}
\begin{example}
    Let \(f(y) = 4y - 5\), \(D = \mathbb{R}\), and \(||\cdot|| = |\cdot|\).
    Then
    \[|f(y_1) - f(y_2)| = |4y_1 - 4y_2| = 4|y_1 - y_2|.\]
    So \(f\) is Lipschitz on \(\mathbb{R}\) with \(Lip(f) = 4\).
\end{example}
\begin{example}
    Let
    \[f(y) = \frac{1}{y - 1}, \quad f'(y) = -\frac{1}{(y - 1)^2}, \quad D = (1, +\infty).\]
    This function is not Lipschitz continuous on \(D\) since as \(y \to 1^+\), the derivative \(f'(y)\) approaches \(-\infty\), and so the ratio \(\frac{|f(y_2) - f(y_1)|}{|y_2 - y_1|}\) approaches \(\infty\) for any \(y_1,y_2 \in D\). Therefore, there is no Lipschitz constant for \(f\) on \(D\).\\
    To make this function Lipschitz continuous, we fix \(\delta > 1\) and define \(D_\delta = (\delta, +\infty)\). For \(y_1,y_2 \in D_\delta\), by the Mean Value Theorem, there exists \(z \in (y_1,y_2)\) such that
    \[f(y_2) - f(y_1) = f'(z)(y_2 - y_1).\]
    So
    \[|f(y_2) - f(y_1)| \le \frac{1}{(z-1)^2}|y_2 - y_1|\le \frac{1}{(\delta-1)^2}|y_2 - y_1|.\]
    Thus \(f\) is Lipschitz on \(D_\delta\) with
    \[Lip(f) = \frac{1}{(\delta - 1)^2}.\]
\end{example}
\begin{remark}[How to choose a Lipschitz constant]
    If \(f\) is differentiable on \(D \subset \mathbb{R}\) and its derivative satisfies
    \[m \le f'(y) \le M \quad \text{for all } y \in D,\]
    then
    \[|f'(y)| \le \max\{|m|,|M|\} \quad \text{for all } y \in D,\]
    and \(f\) is Lipschitz on \(D\) with Lipschitz constant
    \[L = \max\{|m|,|M|\}.\]
    In practice, the Lipschitz constant is any uniform bound on \(|f'|\).
\end{remark}
\begin{example}
    Suppose \(f\) is differentiable and satisfies
    \[-4 \le f'(y) \le 9 \quad \text{for all } y \in \mathbb{R}.\]
    Then
    \[|f'(y)| \le 9 \quad \forall y,\]
    so \(f\) is Lipschitz on \(\mathbb{R}\) with Lipschitz constant \(L = 9\).
\end{example}


\subsubsection{Local Lipschitz continuity}
\begin{definition}[Locally Lipschitz]
    Let \(D \subseteq \mathbb{R}^n\) be open. A function \(f : D \to \mathbb{R}^n\) is called locally Lipschitz if, around every point in \(D\), there is some neighborhood where \(f\) is Lipschitz.
    Equivalently, for every compact set \(K \subset D\), there exists a constant \(L > 0\) such that
    \[\|f(y_1) - f(y_2)\| \le L \|y_1 - y_2\| \quad \text{for all } y_1, y_2 \in K.\]
\end{definition}
\begin{remark}
    Lipschitz continuity and local Lipschitz continuity are not the same.
    \[\text{Lipschitz} \;\Rightarrow\; \text{locally Lipschitz}, \qquad \text{but not conversely.}\]
\end{remark}
\begin{example}[Locally Lipschitz but not Lipschitz]
    \[f(y) = y^2 \in \mathbb{R}.\]
    Since \(f'(y)=2y\) is unbounded on \(\mathbb{R}\), no single constant works on the whole domain, so \(f\) is not Lipschitz.  
    However, on any bounded set \(K=[-M,M]\), \(|f'(y)| \le 2M\), so \(f\) is locally Lipschitz.
\end{example}
\begin{example}[Lipschitz (hence locally Lipschitz)]
    \[f(y) = \sin y.\]
    Since \(|f'(y)| = |\cos y| \leq 1\), $\forall y$, $f$ is Lipschitz on \(\mathbb{R}\), with \(Lip(f) = 1\).
\end{example}
\begin{example}[Continuous but not locally Lipschitz]
    \[f(y) = \sqrt{|y|}.\]
    The derivative is unbounded near \(y = 0\), so no Lipschitz constant exists even locally. Hence \(f\) is not locally Lipschitz.
\end{example}

\subsubsection{Existence and Uniqueness Theorem}
\begin{theorem}[Existence and Uniqueness]
    Let \(D \subseteq \mathbb{R}^n\) be open and let \((a,b)\) be an open interval containing \(t_0\). Consider the IVP
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}\]
    Assume \(f : D \times (a,b) \to \mathbb{R}^n\) is continuous and locally Lipschitz in \(y\).\footnote{\(D \times (a,b)\) denotes the set of all pairs \((y,t)\) with \(y \in D \subset \mathbb{R}^n\) and \(t \in (a,b)\), i.e., all allowed state-time inputs of \(f\).}
    If \(y_0 \in D\), then there exists an open interval \(J\) containing \(t_0\) on which a solution exists.
    Moreover, this solution is unique on \(J\).
\end{theorem}
\begin{problem}[Existence and Uniqueness Theorem]
    Consider the IVP
    \[\begin{cases}y' = \sqrt{1+y^2}+t^2 \\ y(1) = 0.\end{cases}\]
    \begin{enumerate}
        \item Identify $f(y,t)$.
        \item Decide whether the hypotheses of the Existence and Uniqueness Theorem are satisfied.
        \item State clearly what the theorem guarantees about solutions near $t = 1$.
    \end{enumerate}
    Do not solve the ODE. Simply analyze it.
\end{problem}
\marginpar{January 20, 2026}
\begin{remark}[How \(\varepsilon\) is chosen in the theorem]\footnote{In this course, you are only expected to determine \(\varepsilon\) from the theorem, and not to justify the construction further.}
    To apply the Existence and Uniqueness Theorem, we choose \(\varepsilon\) so that the solution remains inside the domain \(D \times (a,b)\).
    Consider the IVP
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}\]
    The theorem guarantees that there exists a unique solution 
    \[y : (t_0-\varepsilon, t_0+\varepsilon) \to \mathbb{R}^n.\]
    Choose \(\alpha > 0\) and \(\delta > 0\) such that
    \[D_{\alpha,\delta} := \{(y,t) : \|y-y_0\| \le \alpha,\ |t-t_0| \le \delta\} \subset D \times (a,b).\]
    Since \(f\) is continuous, define
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} \|f(y,t)\| < \infty.\]
    Then one may take
    \[\varepsilon = \min\!\left(\delta, \frac{\alpha}{M_{\alpha,\delta}}\right).\]
\end{remark}
\begin{problem}[Existence and Uniqueness Theorem 2]
    Consider the initial value problem
    \[\begin{cases}y' = y + 1, \\ y(0) = 1,\end{cases} \qquad \text{with } t_0 = 0,\; y_0 = 1.\]
    Use the Existence and Uniqueness Theorem to determine an interval on which a unique solution is guaranteed to exist. Compute \(\varepsilon\) explicitly using the theorem.
\end{problem}
\begin{problem}[Existence and Uniqueness Theorem 3]
    Consider the initial value problem
    \[\begin{cases}y' = y^2, \\ y(0) = 1.\end{cases}\]
    Use the Existence and Uniqueness Theorem to estimate an interval of existence.
\end{problem}
\begin{problem}[Existence and Uniqueness Theorem 4]
    Consider the first order differential equation
    \[y' + \frac{t}{t^2 - 25}y = \frac{e^t}{t - 9}\]
    For each of the initial conditions below, determine the largest interval $a < t < b$ on which the existence and uniqueness theorem for first order linear differential equations guarantees the existence of a unique solution.
    \begin{itemize}
        \item $y(-7) = -2.1.$
        \item $y(-1.5) = -3.14.$
        \item $y(8.5) = 6.4.$
        \item $y(13) = -0.5.$
    \end{itemize}
\end{problem}
\begin{problem}[LLC]
    Consider the initial value problem
    \[\begin{cases}y' = 3y^{2/3}, \\ y(0) = 0.\end{cases}\]
    Investigate existence and uniqueness.
\end{problem}

\subsubsection{Integral form of solutions}
\marginpar{January 13, 2026}
\begin{lemma}
    A function \(y\) solves the IVP if and only if
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
\end{lemma}
\begin{proof}
    If \(y' = f(y,t)\) and \(y(t_0) = y_0\), then by the Fundamental Theorem of Calculus,
    \[y(t) - y(t_0) = \int_{t_0}^{t} f(y(s),s)ds,\]
    so
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
    Conversely, differentiating the right-hand side gives
    \[y'(t) = f(y(t),t), \quad y(t_0) = y_0.\]
\end{proof}

\subsubsection{Picard operator}
Let \((y_0,t_0) \in D \times (a,b)\). Since this set is open, \(\exists\alpha,\delta > 0\) such that
\[D_{\alpha,\delta} = \{(y,t) : ||y - y_0|| \leq \alpha, |t - t_0| \leq \delta\} \subset D \times (a,b).\]
Define
\[M_{\alpha,\delta} = \sup_{(y,t) \in D_{\alpha,\delta}} ||f(y,t)|| < +\infty.\footnote{The supremum (sup) is the smallest upper bound of a set. Unlike a maximum, it need not be attained. If the largest value is achieved, then sup = max. For example, on $[0,1]$ we have $\sup=\max=1$, while on $(0,1)$ the sup is $1$ but no maximum exists.}\]
Let
\[\epsilon = \min\left(\delta, \frac{\alpha}{M_{\alpha,\delta}}\right), \quad J = (t_0 - \epsilon, t_0 + \epsilon).\]
\begin{definition}[Picard Operator]
    For any function \(y : J \to \mathbb{R}^n\) such that \(y(t_0)=y_0\) and \((y(t),t) \in D_{\alpha,\delta}\), \(\forall t \in J\), define  
    \[T(y)(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
    The map \(T\) is called the Picard operator.
\end{definition}
\begin{example}[Picard operator]
    Consider the IVP $y' = y$, $y(0) = 1$. It's integral form is $y(t) = 1 + \int_0^t y(s)ds$. Define the Picard operator by
    \[T(y)(t) = 1 + \int_0^t y(s)ds.\]
    Start with the simplest function that satisfies the initial condition: $y_0(t) = 1$. Now iterate:
    \[y_1 = T(y_0) = 1 + \int_0^t 1ds = 1 + t,\]
    \[y_2 = T(y_1) = 1 + \int_0^t (1+s)ds = 1+t+\frac{t^2}{2},\]
    \[y_3 = T(y_2) = 1 + t + \frac{t^2}{2} + \frac{t^3}{6}.\]
    Each step adds to the next power of t. We can express this relation as
    \[y(t) = \sum_{k=0}^\infty \frac{t^k}{k!} = e^t.\]
    Indeed, $T(y)(t) = 1 + \int_0^t e^s ds = e^t$, so $T(y)=y$. Therefore, $T$ is an operator that maps a function to a new one, and only the true solution is a fixed point of $T$.
\end{example}
\begin{remark}
    The operator \(T\) takes a function \(y(t)\) and produces a new one. A function \(y\) solves the IVP if and only if \(T(y) = y\), i.e., \(y\) is a fixed point of \(T\).
\end{remark}
\begin{lemma}[Invariant Properties]
    If \(y(t_0)=y_0\) and \((y(t),t) \in D_{\alpha,\delta}\) \(\forall t \in J\), then \(T(y)(t_0)=y_0\) and \((T(y)(t),t) \in D_{\alpha,\delta}\) \(\forall t \in J\).
\end{lemma}\footnote{Basically, If a function stays inside the box, then applying $T$ keeps it inside the box.}
\begin{proof}
    Clearly \(T(y)(t_0)=y_0\). For \(t \in J\),
    \[\|T(y)(t) - y_0\| \le \int_{t_0}^{t} \|f(y(s),s)\|ds \le M_{\alpha,\delta}|t - t_0| \le M_{\alpha,\delta}\epsilon \le \alpha.\]
    Hence \((T(y)(t),t) \in D_{\alpha,\delta}\).
\end{proof}

\subsubsection{Picard Iteration and Existence}
\marginpar{January 15, 2026}
Define $y_0(t) = y_0$ (constant function), which clearly satisfies $y(t_0) = y_0$ (*1) and $(y(t), t) \in D_{\alpha, \delta} \forall t \in J$ (*2).
For $k \geq 1$, define
\[y_k(t) = T(y_{k-1})(t) = y_0 + \int_{t_0}^{t} f(y_{k-1}(s),s)ds.\]
The Picard iterations converge uniformly to a function $y : J \to \mathbb{R}^n$ which satisfies (*1) and (*2), and is a solution of the IVP.
To see the proof, refer to (\textbf{\ref{pf:picard-convergence}}).
\begin{remark}
    Picard iteration is a way to build a solution to an initial value problem step by step. You start with the simplest function that satisfies the initial condition, usually a constant $y_0(t) = y_0$. Then you repeatedly apply the Picard operator, which takes your current guess and “corrects” it by integrating the slope $f(y,t)$ over time. Each iteration produces a slightly better approximation, and as you repeat this process, the sequence of functions converges to the true solution. The final function still starts at the right initial value, stays within the allowed region, and satisfies the differential equation. In essence, Picard iteration turns solving a differential equation into a sequence of manageable approximations that gradually reveal the solution.
\end{remark}

\subsubsection{Uniqueness}
\begin{proof}[Proof of Uniqueness]
    Assume $y(t)$ and $z(t)$ are two solutions of the IVP with the same initial condition. By the Lipschitz property of $f$,
    \[\|y(t)-z(t)\| = \|\int_{t_0}^t f(y(s),s) - f(z(s),s) ds\| \leq \int_{t_0}^t L\|y(s) - z(s)\|ds,\]
    for some Lipschitz constant $L>0$. Define 
    \[g(t) = \int_{t_0}^{t} \|y(s)-z(s)\|ds,\]
    so $g'(t) \leq L g(t)$ and $g(t_0)=0$. Grönwall's idea\footnote{Multiplying by $e^{-L(t-t_0)}$ gives $\frac{d}{dt}(e^{-L(t-t_0)}g(t)) \le 0$, which shows $e^{-L(t-t_0)}g(t)$ is decreasing and thus $g(t) = 0$} tells us this forces $g(t) = 0$ for all $t$, so $\|y(t) - z(t)\| = 0$. Hence, $y(t) = z(t)$, meaning two solutions with the same initial condition cannot diverge, this proves the solution is unique.
\end{proof}

\section{First-Order Scalar Equation}
\marginpar{January 22, 2026}
A first-order scalar differential equation is an equation involving a single scalar function $y(t)$ and its first derivative $y'$, generally expressed as
\[y' = f(y,t).\]
\subsection{First Order Linear Equations}
\begin{definition}[First Order Linear Equation]
    A first-order linear equation is of the form
    \[a_0(t)y' + a_1(t)y = g(t),\]
    where $a_0, a_1, g$ are functions of $t$ and $a_0(t) \neq 0$.
    Dividing by $a_0(t)$ gives the standard form
    \[y' + P(t)y = Q(t), \quad P(t) = \frac{a_1(t)}{a_0(t)}, \quad Q(t) = \frac{g(t)}{a_0(t)}.\]
\end{definition}

\subsubsection{Integrating Factor Method}
\begin{definition}[Integrating Factor]
    An integrating factor is a function $\mu(t)$ such that
    \[\mu(t)y' + \mu(t)P(t)y = \mu(t)Q(t).\]
\end{definition}
\begin{proposition}
    Consider the first-order linear equation
    \[y' + P(t)y = Q(t).\]
    We multiply the equation by a function $\mu(t)$ to be determined:
    \[\mu y' + \mu P(t)y = \mu Q(t).\]
    We require that the left-hand side be the derivative of a product:
    \[\mu y' + \mu P(t)y = \frac{d}{dt}\big(\mu(t)y(t)\big). \tag{*}\]
    The condition (*) holds provided that
    \[\mu'(t) = \mu(t)P(t).\]
    Solving for $\mu$,
    \[ln|\mu(t)| = \int P(t)dt + C, \quad C \in \mathbb{R}\]
    \[\implies |\mu(t)| = e^{C}e^{\int P(t)dt}\]
    \[\therefore \mu(t) = C_1e^{\int P(t)dt}, \quad C_1 = \pm e^{C}.\]
    We can therefore say choose $\mu(t) = e^{\int P(t)dt}$ (integrating factor).
    This is the general solution of the first order linear equation.
\end{proposition}
\begin{example}
    Solve the first-order linear equation 
    \[y' - 2y = 3e^t.\] 
    First, identify the functions $P(t) = -2, \quad Q(t) = 3e^t$. 
    Next, compute the integrating factor $\mu(t) = e^{\int P(t) dt} = e^{-2t}$.
    Multiply through by \(\mu(t)\) to rewrite the left-hand side as a derivative:
    \[e^{-2t}(y' - 2y) = e^{-2t}3e^{t}.\]
    \[\implies \frac{d}{dt}(e^{-2t}y) = 3e^{-t}.\]
    Integrate both sides with respect to \(t\) and solve for $y(t)$
    \[e^{-2t}y = \int 3e^{-t} dt + C = -3e^{-t} + C.\] 
    \[\implies y(t) = e^{2t}(-3e^{-t} + C) = -3e^t + Ce^{2t}.\] 
    Using the initial condition \(y(0) = 1\), we find \(C = 4\), giving the final solution:
    \[\therefore y(t) = -3e^t + 4e^{2t}.\]
\end{example}

\subsubsection{Existence and Uniqueness for First Order Linear Equations}
\begin{theorem}[Existence \& Uniqueness]
    Let there be a first-order linear IVP of the form
    \[y' + P(t)y = Q(t), \quad y(t_0) = y_0.\]
    If \(P\) and \(Q\) are continuous on an interval containing \(t_0\), there exists a unique solution on that interval.
\end{theorem}
This theorem guarantees that any first-order linear equation we solve, including real-world applications like falling objects or mixing problems, has a unique solution on the interval where \(P\) and \(Q\) are continuous.
\begin{example}
    \[\begin{cases}
        y' + \frac{1}{t-1}y = \frac{1}{\cos t} \\
        y(0) = 1
    \end{cases}\]
    Maximal interval: \(J_{max} = (-\frac{\pi}{2}, 1)\)
\end{example}

\subsection{Applications}
\begin{example}[Falling Object with Air Resistance]
    Consider an object falling under gravity while experiencing air resistance proportional to its velocity. The differential equation is
    \[v' + \frac{\gamma}{m}v = g.\]
    Using the integrating factor \(\mu(t) = e^{\frac{\gamma}{m}t}\), we rewrite the equation as
    \[\frac{d}{dt}(\mu v) = g\mu \implies v = \frac{gm}{\gamma} + Ce^{-\frac{\gamma}{m}t}.\]
    Applying the initial condition \(v(0) = \frac{gm}{\gamma}\) gives \(C = 0\), so the velocity reaches a constant terminal value:
    \[v(t) = \frac{gm}{\gamma}.\]
\end{example}
\begin{example}[Mixing Problem]
    A tank initially contains 100 L of brine with \(y_0\) g of dissolved salt. Pure salt solution enters at 50 g/L at rate \(R\) L/s, and the well-mixed solution leaves at the same rate. Let \(y(t)\) be the amount of salt at time \(t\). The equation is
    \[y' = 50R - \frac{R}{100}y \implies y' + \frac{R}{100}y = 50R, \quad y(0) = y_0.\]
    Using the integrating factor \(\mu(t) = e^{\frac{R}{100}t}\), we solve:
    \[y(t) = 5000 + (y_0 - 5000)e^{-\frac{R}{100}t}.\]
    The solution shows that over time the salt concentration approaches a stable threshold of 5000 g.
\end{example}

\subsection{Separable Equations}
\marginpar{January 27, 2026}
\begin{definition}
    Let \(f(x,y)\) be a function of two variables. Then \(f\) is separable if there exist two functions \(f_1(x)\) and \(f_2(y)\) such that \(f(x,y) = f_1(x)f_2(y)\).
\end{definition}
\begin{example}
    This following ODE is separable
    \[\frac{dy}{dx} = -\frac{x}{y}\]
    \[f_1(x) = x,\quad f_2(y) = -\frac{1}{y}\]
\end{example}
\begin{example}
    The following ODE
    \[\frac{dy}{dx} = x^2 + y^2\]
    is not separable because it is not in the form \(f_1(x) \cdot f_2(y)\).
\end{example}

\subsubsection{General Method for Separable Equations}
Defining the general form of a separable ODE to be
\[\frac{dy}{dx} = f_1(x)f_2(y).\]
We can rewrite as
\begin{equation}
    \label{SeparableODE}
    M(x) + N(y)\frac{dy}{dx} = 0,
\end{equation}
where $M(x) = -f_1(x)$ and $N(y) = \frac{1}{f_2(y)}$.

Consider $H_1(x)$ and $H_2(y)$ as antiderivatives of $M(x)$ and $N(y)$, respectively.  
Using the Chain Rule,
\[\frac{d}{dx}H_2(y(x)) = H_2'(y)\frac{dy}{dx}.\] 
The ODE can be written as
\[H_1'(x) + H_2'(y)\frac{dy}{dx} = 0 \implies \frac{d}{dx}(H_1(x) + H_2(y(x))) = 0.\] 
Therefore,
\[H_1(x) + H_2(y(x)) = C,\]
which is the implicit general solution.

In integral form, the solution is
\[\int \frac{dy}{f_2(y)} = \int f_1(x) dx + C, \quad C \in \mathbb{R},\]
which provides a systematic way to solve any separable equation by integrating the $x$-part and $y$-part separately.

\begin{example}[Logistic Equation]
    Consider the logistic equation
    \[\frac{dN}{dt} = rN\Big(1 - \frac{N}{K}\Big).\]
    This ODE is separable. Using partial fractions:
    \[\frac{K}{N(K-N)} = \frac{1}{N} + \frac{1}{N-K},\]
    so
    \[\int \frac{dN}{N} + \int \frac{dN}{N-K} = \int r dt \implies \ln|\frac{N}{N-K}| = rt + C.\]
    Solving for $N(t)$ gives
    \[N(t) = \frac{KN_0}{N_0 + (K-N_0)e^{-rt}},\]
    where $N_0 = N(0)$.  

    This shows that $N(t)$ approaches the carrying capacity $K$ as $t \to \infty$, with special cases $N_0 = 0 \implies N(t) = 0$ and $N_0 = K \implies N(t) = K$ for all $t$.
\end{example}

\subsection{Potential Functions and Integral Curves}
\begin{definition}[Potential Function]
    A potential function is a function $\Psi(x,y)$ whose level curves define the solutions of a differential equation. For separable equations, we can take
    \begin{equation}
        \Psi(x,y) = H_1(x) + H_2(y),
    \end{equation}
    where $H_1$ and $H_2$ are antiderivatives of $M(x)$ and $N(y)$ from \textup{\eqref{SeparableODE}}.
\end{definition}
\begin{definition}[Level Curve]
    A level curve of a potential function is a curve along which the potential function is constant:
    \[\Psi(x,y) = C.\]
    These curves are tangent to the vector field defined by the ODE and represent the general solution in implicit form.
\end{definition}
\begin{definition}[Integral Curve]
    An integral curve is a level curve of the potential function that passes through a specific initial condition $(x_0, y_0)$.  
    This gives the unique solution curve corresponding to that initial condition.
\end{definition}
An integral curve is a geometric object that contains (possibly many) solutions. The unique integral curve that contains the initial condition $y(x_0) = y_0$ is defined by
\[\Gamma_{(x_0,y_0)} = \{(x,y) | \Psi(x,y) = \Psi(x_0,y_0) = C(x_0,y_0)\},\]
and in the separable case,
\[\frac{d}{dx}\Big[\int f_1(x)dx + \int \frac{dy}{f_2(y)}\Big] = 0.\]
\begin{remark}
    For a separable equation, the general solution $\Psi(x,y) = C$ describes all possible solution curves, and picking a particular $C$ gives the integral curve passing through a given point.
\end{remark}
\begin{example}
    Consider the separable differential equation
    \[\frac{dy}{dx} = -\frac{x}{y}, \qquad y \neq 0.\]
    Separating variables gives $y\,dy = -x\,dx.$ Integrating both sides,
    \[\int y\,dy + \int x\,dx = C \quad \implies \quad \frac{y^2}{2} + \frac{x^2}{2} = C.\]
    Define the potential function $\Psi(x,y) = \frac{x^2}{2} + \frac{y^2}{2}.$ The general solution is given implicitly by the level curves $\Psi(x,y) = C,$ which are circles centered at the origin. These level curves are the integral curves of the differential equation. For the initial condition $y(0) = 2$, $\Psi(0,2) = 2$, so the unique integral curve is
    \[\frac{x^2}{2} + \frac{y^2}{2} = 2 \quad \implies \quad y(x) = \sqrt{4 - x^2}, \qquad J_{\text{max}} = (-2,2).\]
\end{example}

\subsection{Exact Equations}
\marginpar{January 29, 2026}
\begin{definition}[Exact Equation]
    The first-order ODE \textup{\eqref{SeparableODE}} is called \emph{exact} on a domain $D$ if there exists a function $\Psi(x,y)$ (a potential function) such that
    \[\frac{\partial \Psi}{\partial x} = M \quad \text{and} \quad \frac{\partial \Psi}{\partial y} = N \qquad \forall (x,y)\in D.\]
    In this case, $d\Psi = M\,dx + N\,dy$, so along any solution $y(x)$ we have
    \[\frac{d}{dx}\big(\Psi(x,y(x))\big) = 0.\]
    Hence $\Psi$ is constant along solutions, and the general solution is given implicitly by
    \[\Psi(x,y) = C,\]
    whose level curves are the integral curves of the ODE.
\end{definition}

\begin{theorem}
    Let $D \subset \mathbb{R}^2$ be an open, simply connected domain\footnote{Meaning there are no holes in the domain.} in the plane. Suppose $M$, $N$, and their partial derivatives $M_y = \frac{\partial M}{\partial y}$ and $N_x = \frac{\partial N}{\partial x}$ are continuous on $D$.
    Then the first-order equation \textup{\eqref{SeparableODE}} is exact on $D$ if and only if $M_y(x,y) = N_x(x,y)$ for all $(x,y)\in D$.
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Suppose the equation is exact. Then there exists a potential function $\Psi(x,y)$ such that
    \[M = \Psi_x, \qquad N = \Psi_y.\]
    Differentiating,
    \[M_y = (\Psi_x)_y = \Psi_{xy}, \qquad N_x = (\Psi_y)_x = \Psi_{yx}.\]
    Since mixed partial derivatives are equal for $C^2$ functions,
    \[\Psi_{xy} = \Psi_{yx},\]
    hence $M_y = N_x$.

    ($\Leftarrow$) Conversely, if $M_y = N_x$ on a simply connected domain $D$ and the partial derivatives are continuous, then there exists a function $\Psi$ such that $\Psi_x = M$ and $\Psi_y = N$. Thus the equation is exact.\footnote{The construction is shown for rectangles in the notes; the general case follows similarly.}
\end{proof}
\begin{example}
    Solve the differential equation
    \[y\cos x + 2x e^{y} + (\sin x + x^{2} e^{y} - 1)\frac{dy}{dx} = 0.\]
    Write it in the form $M(x,y)\,dx + N(x,y)\,dy = 0$, where
    \[M(x,y) = y\cos x + 2x e^{y}, \qquad N(x,y) = \sin x + x^{2} e^{y} - 1.\]
    Since $M, N$ and their partial derivatives are continuous on 
    $\mathbb{R}^2$, we test exactness:
    \[M_y = \cos x + 2x e^{y}, \qquad N_x = \cos x + 2x e^{y}.\]
    Thus $M_y = N_x$, so the equation is exact. Hence there exists a potential function $\Psi$ such that
    \[\Psi_x = M, \qquad \Psi_y = N.\]
    Integrate $\Psi_x = M$ with respect to $x$:
    \[\Psi(x,y) = \int (y\cos x + 2x e^{y})\,dx + c(y) = y\sin x + x^{2} e^{y} + c(y),\]
    where $c(y)$ depends only on $y$. Differentiate with respect to $y$:
    \[\Psi_y = \sin x + x^{2} e^{y} + c'(y).\]
    Set this equal to $N$:
    \[\sin x + x^{2} e^{y} + c'(y) = \sin x + x^{2} e^{y} - 1,\]
    giving
    \[c'(y) = -1 \quad \Rightarrow \quad c(y) = -y.\]
    Therefore the potential function is
    \[\Psi(x,y) = y\sin x + x^{2} e^{y} - y,\]
    and the general solution is given implicitly by
    \[\Psi(x,y) = C.\]
\end{example}
\begin{remark}
    Every separable equation is exact on a simply connected domain.
    Indeed, a separable equation can be written as
    \[M(x)\,dx + N(y)\,dy = 0,\]
    where $M$ depends only on $x$ and $N$ only on $y$. Hence
    \[M_y = 0 = N_x,\]
    so the exactness condition $M_y = N_x$ is automatically satisfied.
\end{remark}

\subsubsection{Integrating Factors for Exact Equations}
\marginpar{February 3, 2026}
Consider the first-order equation
\begin{equation}
    M(x,y)\,dx + N(x,y)\,dy = 0,
\end{equation}
and suppose it is not exact, so that $M_y \neq N_x$. We try to multiply the equation by a function $\mu(x,y)$ (the integrating factor) to obtain
\[\mu(x,y)M(x,y)\,dx + \mu(x,y)N(x,y)\,dy = 0.\]
Our goal is to choose $\mu$ so that this new equation is exact.
Hence we require $(\mu M)_y = (\mu N)_x$. Using the product rule,
\[(\mu M)_y = \mu_y M + \mu M_y, \qquad (\mu N)_x = \mu_x N + \mu N_x.\]
Therefore $\mu$ must satisfy
\[\mu_y M + \mu M_y = \mu_x N + \mu N_x.\]
This is generally a partial differential equation for $\mu$.

To simplify, we often look for integrating factors that depend on only
one variable.

\textbf{Case 1:} $\mu = \mu(x)$.

Then $\mu_y = 0$, and the condition becomes
\[\mu M_y = \mu' N + \mu N_x.\]
Rearranging,
\[\mu' = \mu \frac{N_x - M_y}{N}.\]
If the quantity $\frac{N_x - M_y}{N}$ depends only on $x$, this is a separable ODE for $\mu(x)$, which we can solve. After finding $\mu$, multiplying the original equation by $\mu$ makes it exact.
\begin{example}
    Solve
    \[(3xy + y^2)\,dx + (x^2 + xy)\,dy = 0.\]
    Identify
    \[M(x,y) = 3xy + y^2, \qquad N(x,y) = x^2 + xy.\]
    Test exactness:
    \[M_y = 3x + 2y, \qquad N_x = 2x + y.\]
    Since $M_y \ne N_x$, the equation is not exact. Therefore we seek an integrating factor $\mu(x,y)$ that makes the equation exact. Compute
    \[\frac{M_y - N_x}{N} = \frac{(3x+2y)-(2x+y)}{x^2+xy} = \frac{x+y}{x(x+y)}
    = \frac{1}{x}.\]
    This depends only on $x$, so we can solve the ODE for $\mu(x)$.
    \[\frac{d\mu}{dx} = \mu \frac{1}{x}.\]
    Solving this not so difficult ODE, we find that $\mu(x) = x$. Going back to our original equation, multiply by $x$:
    \[(3x^2y + xy^2)\,dx + (x^3 + x^2y)\,dy = 0.\]
    We identify
    \[M = 3x^2y + xy^2, \qquad N = x^3 + x^2y,\]
    and
    \[M_y = 3x^2 + 2xy, \qquad N_x = 3x^2 + 2xy,\]
    so the equation is exact since $M_y = N_x$. We have to now find a potential function. Integrate $M$ with respect to $x$:
    \[\Psi(x,y) = \int (3x^2y + xy^2)\,dx + c(y) = x^3y + \frac{x^2y^2}{2} + c(y).\]
    Differentiate:
    \[\Psi_y = x^3 + x^2y + c'(y).\]
    Matching with $N$ gives $c'(y)=0$, so $c(y)=0$. Therefore the general implicit solution is
    \[\Psi(x,y) = x^3y + \frac{x^2y^2}{2} = C.\]
\end{example}


\section{Systems of Linear Equations}
\marginpar{February 3, 2026}
Many problems involve several unknown functions that evolve together and influence one another. Instead of a single differential equation, we therefore study systems of first-order linear ODEs. Writing the unknown functions as a vector allows us to treat the entire system using matrix algebra and apply the tools of linear algebra (matrices, eigenvalues, and fundamental matrices).

We consider systems of the form
\begin{equation}
    \label{eq:ode-system}
    y'(t) = A(t)y(t) + r(t), \quad t \in Q,
\end{equation}
where $y(t)$ is the unknown vector, $A(t)$ is a matrix of coefficients, $r(t)$ is a forcing term, and $Q \subseteq \mathbb{R}$. Explicitly,
\[y(t) = \begin{pmatrix}y_1(t)\\\vdots\\y_n(t)\end{pmatrix} \in \mathbb{R}^n, \quad A(t) \in M_n(\mathbb{R}), \quad
r(t) = \begin{pmatrix}r_1(t)\\\vdots\\r_n(t)\end{pmatrix},\]

\subsection{From higher order scalar ODEs to linear systems}
\begin{example}
    Convert the third order equation
    \[y''' - 2y'' + 4y' - 5y = \sin t + t^3\]
    into a first order system.
    Let
    \[y_1 = y, \quad y_2 = y', \quad y_3 = y''.\]
    Then
    \[\begin{pmatrix}y_1'\\y_2'\\y_3'\end{pmatrix} = \begin{pmatrix}0 & 1 & 0\\0 & 0 & 1\\5 & -4 & 2\end{pmatrix}\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix} + \begin{pmatrix}0\\0\\\sin t + t^3\end{pmatrix}.\]
    Thus the equation is written in the form \(y' = Ay + r\), with \(J_{\max}=\mathbb{R}\).
\end{example}

\subsection{Theory of First Order Linear Systems}
\begin{theorem}[Existence and uniqueness]
    If every entry of \(A(t)\) and \(r(t)\) is continuous on an interval \(Q\), then for any
    \[t_0 \in Q, \quad y_0 \in \mathbb{R}^n,\]
    the IVP
    \[\begin{cases}y'(t) = A(t)y(t) + r(t), \\ y(t_0) = y_0\end{cases}\]
    has a unique solution on all of \(Q\). Hence \(J_{\max}=Q\).
\end{theorem}
\begin{definition}
    If \(r(t)=0\), the system is homogeneous. Otherwise it is non-homogeneous.
\end{definition}

For a homogeneous system 
\begin{equation}
    \label{eq:ode-system-homogeneous}
    y'(t)=A(t)y(t),
\end{equation}
the set of all solutions forms a vector space: it contains the zero function, and any linear combination of solutions is also a solution \{\ref{POS}\}. To describe this space concretely, we select \(n\) linearly independent solutions; arranging them as the columns of a matrix gives a fundamental matrix, which allows every solution to be expressed as a linear combination of these columns.

The non-homogeneous system \(y' = A(t)y + r(t)\) can be solved using a fundamental matrix \(\Phi(t)\) of the homogeneous system:
\[y(t) = \Phi(t) c + \Phi(t) \int_{t_0}^t \Phi(s)^{-1} r(s)ds,\]
where \(c \in \mathbb{R}^n\) is determined by the initial condition \(y(t_0) = y_0\).

\subsection{Structure of the Solution Space}%to read
\begin{definition}[Linear Independence]
    Functions \(y_1, \dots, y_n\) in the solution set \(S\)\footnote{\(y_1, \dots, y_n\) are solutions of \(y' = A(t)y\)} are linearly independent if
    \[c_1 y_1(t) + \cdots + c_n y_n(t) = 0 \quad \forall t \in Q \implies c_1 = \cdots = c_n = 0.\]
\end{definition}
\begin{theorem}[Solution Set]
    The solution set \(S\) is an \(n\)-dimensional vector subspace of \(C^1(Q)\).
\end{theorem}
\begin{proof}To prove that $S$ is a vector space of dimension \(n\), we show the following:
    \textbf{Step 1: Vector space.} If \(y_1,y_2 \in S\) and \(\alpha_1,\alpha_2 \in \mathbb{R}\),
    \[
    (\alpha_1 y_1 + \alpha_2 y_2)' = \alpha_1 y_1' + \alpha_2 y_2' = A(t)(\alpha_1 y_1 + \alpha_2 y_2),
    \]
    so any linear combination is in \(S\), and the zero function is also in \(S\).

    \textbf{Step 2: Construct a basis.} Let \(e_1,\dots,e_n\) be the standard basis of \(\mathbb{R}^n\), and let \(y_i\) be the unique solution of
    \[
    \begin{cases}
    y' = A(t)y, \\ 
    y(t_0) = e_i
    \end{cases}
    \]
    for \(i=1,\dots,n\). Define \(B = \{y_1,\dots,y_n\}\). By uniqueness, any solution \(z \in S\) can be written as a linear combination of the \(y_i\), so \(B\) spans \(S\). Evaluating a linear combination at \(t_0\) shows the \(y_i\) are independent. Hence, \(B\) is a basis and \(\dim S = n\).
\end{proof}

\marginpar{February 5, 2026}
\begin{definition}[Matrix Solution]
    A matrix solution $y(t)$ of the equation
    \begin{equation}
        \label{eq:ode-system-matrix}
        y' = A(t)y
    \end{equation}
    is an $n \times n$ matrix function whose columns are solutions to \textup{\eqref{eq:ode-system-matrix}}.
\end{definition}
\begin{definition}[Principle of Superposition]
    \label{POS}
    The principle of superposition states that if \(y_1(t), \dots, y_n(t)\) are solutions of \(y' = A(t)y\), then any linear combination of these solutions is also a solution.
\end{definition}
By the POS (principle of superposition), for any fixed vector
\[C = \begin{pmatrix}C_1\\C_2\\\vdots\\C_n\end{pmatrix} \in \mathbb{R}^n, \quad y(t)c = c_1 y_1(t) + c_2 y_2(t) + \cdots + c_n y_n(t) \in S.\]
\begin{proposition}
    Let $y(t)$ be a matrix solution of \textup{\eqref{eq:ode-system-matrix}}. Either
    \[\det(y(t)) \neq 0 \quad \forall t \in Q\]
    or
    \[\det(y(t)) = 0.\]
\end{proposition}
\begin{proof}
    Assume there exists $t_0 \in Q$ such that $\det(y(t_0)) = 0$. Then $y(t_0)$ is not invertible. Hence, by linear algebra, there exists a nonzero vector $v \in \mathbb{R}^n$ such that
    \[y(t_0)v = 0.\]
    Define
    \[y_1(t) := y(t)v.\]
    Since each column of $y(t)$ is a solution of $y' = A(t)y$, any linear combination of the columns is also a solution. Therefore $y_1(t)$ is a solution, i.e. $y_1 \in S$.
    Also define the zero solution
    \[y_2(t) := 0,\text{\footnote{":=" means 'is defined to be'}}\]
    which satisfies $y_2' = A(t)y_2$, so $y_2 \in S$. Now,
    \[y_1(t_0) = y(t_0)v = 0 = y_2(t_0).\]
    Thus $y_1$ and $y_2$ are two solutions with the same initial condition. By the existence and uniqueness theorem,
    \[y_1(t) = y_2(t) \quad \text{for all } t \in Q.\]
    Hence
    \[y(t)v = 0 \quad \text{for all } t.\]
    Therefore the columns of $y(t)$ are linearly dependent for every $t$, which implies
    \[\det(y(t)) = 0 \quad \text{for all } t \in Q.\]
\end{proof}
\begin{definition}[Wronskian]
    \[w(t) = w(y_1,\ldots,y_n)(t) = \det(y(t))\]
    is called the Wronskian of $y_1,\ldots,y_n$.
\end{definition}
\begin{definition}[Fundamental Matrix Solution]
    A matrix solution with nonvanishing\footnote{Nonvanishing in this context means not equal to zero.} Wronskian is called a fundamental matrix solution.
\end{definition}
\begin{theorem}[Liouville's formula]
    Let $y(t)$ be a matrix solution to \textup{\eqref{eq:ode-system-matrix}} satisfying the initial condition $y(t_0) = y_0 \in M_n(\mathbb{R})$. Then
    \[\det(y(t)) = \det(y_0)e^{\int_{t_0}^t \operatorname{tr}(A(s))\,ds}.\]
    Here,
    \[\operatorname{tr}(M) = \text{sum of the diagonal entries of } M \in M_n(\mathbb{R}).\]
\end{theorem}
\begin{remark}
    For the homogeneous system \textup{\eqref{eq:ode-system-matrix}}, the goal is to find $n$ linearly independent solutions
    \[y_1, y_2, \ldots, y_n\]
    such that $W(t_0) \neq 0$.
\end{remark}
Since $y_1,\ldots,y_n$ are linearly independent, they form a basis for the solution space. Therefore every solution has the form
\[y(t) = c_1 y_1(t) + c_2 y_2(t) + \cdots + c_n y_n(t), \qquad c_1,\ldots,c_n \in \mathbb{R}.\]
Given an initial condition
\[y(t_0) = y_0 \in \mathbb{R}^n,\]
\[y(t_0) = c_1 y_1(t_0) + \cdots + c_n y_n(t_0) = \begin{pmatrix}| & | & | \\ y_1(t_0) & y_2(t_0) & y_n(t_0) \\ | & | & |\end{pmatrix}\begin{pmatrix}c_1\\c_2\\\vdots\\c_n\end{pmatrix} = y_0 = \begin{pmatrix}y_0'\\y_0''\\\vdots\\y_0^{(n)}\end{pmatrix}.\]
The unique $(c_1,\ldots,c_n)$ is given by
\[\begin{pmatrix}c_1\\c_2\\\vdots\\c_n\end{pmatrix} = \begin{pmatrix}| & | \\y_1(t_0) & y_n(t_0) \\| & |\end{pmatrix}^{-1}\begin{pmatrix}y_0'\\y_0''\\\vdots\\y_0^{(n)}\end{pmatrix}.\]

\subsection{Linear homogeneous systems with constant coefficients}
Consider \eqref{eq:ode-system-matrix}, where $A \in M_n(\mathbb{R})$ with constant entries. We look for solutions of the form
\begin{equation}
    \label{eq:exp-ansatz}
    y(t) = e^{\lambda t}u
\end{equation}
where $u$ is a constant vector.\footnote{Recall the analogous scalar case $a_0 y^{(n)} + a_1 y^{(n-1)} + \cdots + a_{n-1} y' + a_n y = 0$, where we look for solutions of the form $e^{rt}$.}
If we substitute \eqref{eq:exp-ansatz} into \eqref{eq:ode-system-matrix}, we get
\begin{equation}
    \label{eq:ode-system-exp}
    y'(t) = \frac{d}{dt}(e^{\lambda t}u) = e^{\lambda t}(\lambda u),
\end{equation}
and
\begin{equation}
    \label{eq:ode-system-exp-A}
    Ay = e^{\lambda t}(Au).
\end{equation}
Hence if we combine both equations, we get
\begin{equation}
    \label{eq:ode-system-exp-A-2}
    e^{\lambda t}(\lambda u) = e^{\lambda t}(Au) \implies Au = \lambda u.
\end{equation}
In other words, $\lambda$ is an eigenvalue of $A$ and $u$ is a corresponding eigenvector.
\begin{definition}[Kernel]
    Given a matrix $A \in M_n(\mathbb{C})$, define
    \[\ker A = \{u \mid Au = 0\},\]
    called the kernel (or null space) of $A$.
\end{definition}
Equation \eqref{eq:ode-system-exp-A-2} is equivalent to
\[Au - \lambda u = 0 \iff (A - \lambda I)u = 0,\]
where $I$ is the identity matrix in $M_n(\mathbb{R})$.
\begin{definition}[Characteristic Polynomial]
    Let $A \in M_n(\mathbb{R})$. The characteristic polynomial of $A$, denoted $p(\lambda)$, is defined to be
    \[p(\lambda) = \det(A - \lambda I)\]
\end{definition}
\begin{definition}[Eigenvalue]
    An eigenvalue of $A$ is a root of $p(\lambda)$.
\end{definition}
\begin{definition}[Spectrum]
    The set
    \[\sigma(A) = \{\lambda \mid p(\lambda) = 0\}\]
    is called the spectrum of $A$.
\end{definition}
\begin{example}
    Consider
    \[y' = Ay, \qquad y = \begin{pmatrix}y_1\\y_2\end{pmatrix}, \qquad A = \begin{pmatrix}1 & 3\\ 3 & 1\end{pmatrix}.\]
    To solve the system, we first find the eigenvalues and eigenvectors of $A$. Define the characteristic polynomial to be
    \[p(\lambda) = \det(A - \lambda I) = \begin{vmatrix}1-\lambda & 3\\3 & 1-\lambda\end{vmatrix} = (1-\lambda)^2 - 9.\]
    Solving $(1-\lambda)^2 - 9 = 0$, gives us two eigenvalues $\lambda_1 = -2$ and $\lambda_2 = 4$. Hence the spectrum of $A$ is
    \[\sigma(A) = \{-2,4\}.\]
    To find the eigenvectors, we need to solve $(A-\lambda I)u = 0$.

    For $\lambda_1 = -2$:
    \[(A-\lambda I)u_1 = 0 \implies \begin{pmatrix}3 & 3\\ 3 & 3\end{pmatrix}\begin{pmatrix}a\\b\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix} \implies b = -a.\]
    You can pick any values of $a$, and $b$ is the negative of $a$; choose $a=1$:
    \[u_1 = \begin{pmatrix}1\\-1\end{pmatrix}.\]

    For $\lambda_2 = 4$, solving $(A-4I)u_2=0$ similarly yields
    \[u_2 = \begin{pmatrix}1\\1\end{pmatrix}.\]

    Each eigenpair $(\lambda,u)$ produces a solution of the system of the form $y(t)=e^{\lambda t}u$. Hence
    \[y_1(t) = e^{-2t}\begin{pmatrix}1\\-1\end{pmatrix} \in S, \qquad y_2(t) = e^{4t}\begin{pmatrix}1\\1\end{pmatrix} \in S.\]

    The wronskian of $y_1$ and $y_2$ is
    \[w(t) = \det\begin{pmatrix}e^{-2t} & e^{4t}\\- e^{-2t} & e^{4t}\end{pmatrix}.\]
    Evaluating at $t=0$,
    \[w(0) = \begin{vmatrix}1 & 1\\-1 & 1\end{vmatrix} = 2 \neq 0,\]
    so $y_1$ and $y_2$ are linearly independent. Therefore they form a basis of the solution space,
    \[S = \langle \{y_1,y_2\} \rangle,\]
    and the general solution is
    \[y(t) = c_1 e^{-2t}\begin{pmatrix}1\\-1\end{pmatrix} +
    c_2 e^{4t}\begin{pmatrix}1\\1\end{pmatrix}.\]
\end{example}

\section{Second and Higher-Order Scalar Linear Equations}
\section{Stability, Phase Portraits and Orbits}
\section{Laplace Transform}
\section{Power Series Solutions and Numerical Methods}


\section{Appendix}
\subsection{Picard Iterations Convergence}
\begin{proof}[Proof of Picard Iterations Convergence]
    \label{pf:picard-convergence}
    Pick $t \in [t_0, t_0 + \epsilon]$ (the proof is similar for $t \in [t_0 - \epsilon, t_0]$).
    The goal is to show that $\{y_k(t)\}_{k=0}^{\infty}$ is a Cauchy sequence\footnote{Cauchy sequence is a sequence that has a limit in a metric space $\mathbb{R}^n$.} in $\mathbb{R}^n$.\footnote{Evaluated on compacted cylinder, understanding epsilon, the L Lipschitz constant coming from somewhere, not on the analysis background such as the Banach fixed point theorem.}
    We prove by induction that
    \[(**)\quad ||y_m(t) - y_{m - 1}(t)|| \leq L^{m - 1}M_{\alpha,\delta} \frac{(t - t_0)^m}{m!},\:\forall m \geq 1.\]
    \textbf{Base case \(m=1\):}
    \[||y_1(t) - y_0(t)|| = \left\| \int_{t_0}^{t} f(y_0,s)ds \right\| \leq \int_{t_0}^{t}||f(y_0,s)||ds \leq M_{\alpha,\delta}|t - t_0| \leq \alpha.\]
    \textbf{Induction step:}
    \[||y_{m + 1}(t) - y_m(t)|| \leq \int_{t_0}^{t}||f(y_m(s),s) - f(y_{m-1}(s),s)||ds.\]
    Since $D_{\alpha,\delta}$ is compact and $f$ is Lipschitz on $D_{\alpha,\delta}$, there exists $L$ such that
    \[||f(x,t) - f(y,t)|| \leq L||x - y||.\]
    Thus,
    \[\leq L \int_{t_0}^{t}||y_m(s) - y_{m - 1}(s)||ds.\]
    Using (**),
    \[\leq L^m M_{\alpha,\delta} \frac{1}{(m-1)!} \int_{t_0}^{t}(s - t_0)^{m-1}ds
    = L^m M_{\alpha,\delta} \frac{(t - t_0)^m}{m!}.\]
    Hence (**) holds.
    In particular, for all $\rho \geq 1$,
    \[||y_\rho(t) - y_{\rho-1}(t)|| \leq M_{\alpha,\delta} \frac{(L(t - t_0))^{\rho}}{(\rho)!} < \frac{M_{\alpha,\delta}}{L}\frac{(L\epsilon)^\rho}{\rho!}.\]
    Let $m,p \geq 1$:
    \[||y_{m+p}(t) - y_{m+1}(t)|| \leq \sum_{k=1}^{p-1}||y_{m+k+1}(t) - y_{m+k}(t)||.\]
    So,
    \[< \frac{M_{\alpha,\delta}}{L}\sum_{j=m+2}^{m+p}\frac{(L\epsilon)^j}{j!}.\]
    Since \(e^{L\epsilon} = \sum_{j=0}^{\infty}\frac{(L\epsilon)^j}{j!}\) converges,
    \[\to_{m,p \to +\infty} 0.\]
    Thus $\{y_k(t)\}$ is Cauchy and converges to $y(t)$.
    Taking limits in the iteration,
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
\end{proof}

\section{Solutions}
\begin{solution}[System of First Order ODEs]
    Define new variables:
    \[y_1 = y, \quad y_2 = y', \quad y_3 = y''.\]
    Now we can rewrite the third order ODE into a first order system:
    \[\begin{cases}
        y_1' = y_2\\
        y_2' = y_3\\
        y_3' = -4y_2 + y_1
    \end{cases}\]
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem]
    We have the IVP
    \[\begin{cases}y' = \sqrt{1 + y^2} + t^2 \\ y(1) = 0\end{cases}\]
    \begin{enumerate}
        \item The function is simply $f(y,t) = \sqrt{1 + y^2} + t^2$.
        \item Check the hypotheses:
        \begin{itemize}
            \item $f$ is continuous $\forall y \in \mathbb{R},\:t \in \mathbb{R}$ because square root of $1 + y^2$ and $t^2$ are continuous everywhere.
            \item Check local Lipschitz in $y$: $\frac{\partial f}{\partial y} = \frac{y}{\sqrt{1 + y^2}}$, which is continuous $\forall y$, and $|\frac{y}{\sqrt{1 + y^2}}| \leq 1$. Therefore $f$ is locally (and globally, but E \& U only cares about local Lipschitz) Lipschitz in $y$ with $Lip(f) = 1$. So the hypotheses of E \& U th. are satisfied in $\mathbb{R} \times \mathbb{R}$.
        \end{itemize}
        \item Since $f$ is continuous and locally Lipschitz in $y$, and since $y(1) = 0$ with $0 \in \mathbb{R}$, the Existence and Uniqueness Theorem guarantees that there exists an open interval $J$ containing $t = 1$ on which a solution exists, and this solution is unique on $J$.
    \end{enumerate}
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 2]
    We write
    \[f(y,t) = y + 1.\]
    Since \(f\) is a \(C^1\) function, it is locally Lipschitz in \(y\), and the Existence and Uniqueness Theorem applies.
    The domain is
    \[f : D \to \mathbb{R}, \qquad D = \mathbb{R} = (-\infty, +\infty).\]
    Since \(f\) is defined everywhere, there are no constraints on \(\alpha\) and \(\delta\).
    Define
    \[D_{\alpha,\delta} = \{(y,t) : \|y-1\| \le \alpha,\ |t-0| \le \delta\} = [1-\alpha,1+\alpha] \times [-\delta,\delta] \subset \mathbb{R} \times \mathbb{R}.\]
    Then
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} \|f(y,t)\| = \sup_{y\in[1-\alpha,1+\alpha]} |y+1| = 2+\alpha.\]
    The theorem allows us to take
    \[\varepsilon = \min\!\left(\delta, \frac{\alpha}{2+\alpha}\right).\]
    Pick \(\alpha = 1\), \(\delta = 1\):
    \[\varepsilon = \min\!\left(1, \frac{1}{3}\right) = \frac{1}{3}.\]
    Therefore, there exists a unique solution on \(\left(-\frac{1}{3}, \frac{1}{3}\right)\).
    Pick \(\alpha = 3\), \(\delta = 2\):
    \[\varepsilon = \min\!\left(2, \frac{3}{5}\right) = \frac{3}{5}.\]
    Therefore, there exists a unique solution on \(\left(-\frac{3}{5}, \frac{3}{5}\right)\).
    Since there are no constraints on \(\alpha\) and \(\delta\), we can make \(\varepsilon\) as large as we want. Hence, the solution exists and is unique on \(\mathbb{R}\).
    The “maximal” time interval guaranteed by the Existence and Uniqueness Theorem is
    \[J = (-1,1).\]
    In Chapter 2 we will see that the explicit solution is
    \[y(t) = 2e^t - 1.\]
    In fact, the maximal interval on which the solution is defined is
    \[J_{\max} = \mathbb{R}.\]
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 3]
    Since \(f(y)=y^2\) is \(C^1\), it is locally Lipschitz, so a solution exists and is unique locally.
    On \(D_{\alpha,\delta} = [1-\alpha,1+\alpha] \times [-\delta,\delta]\),
    \[M_{\alpha,\delta} = \sup_{y \in [1-\alpha,1+\alpha]} |y^2| = (1+\alpha)^2.\]
    Thus,
    \[\varepsilon = \min\!\left(\delta, \frac{\alpha}{(1+\alpha)^2}\right).\]
    Define
    \[h(\alpha) = \frac{\alpha}{(1+\alpha)^2}, \qquad h'(\alpha) = \frac{1-\alpha}{(1+\alpha)^3}.\]
    Setting \(h'(\alpha)=0\) gives \(\alpha=1\).
    Pick \(\alpha=1\), \(\delta = 104073\):
    \[\varepsilon = \frac{1}{4}.\]
    Therefore, there exists a unique solution
    \[y : \left(-\frac14,\frac14\right) \to \mathbb{R}.\]
    Chapter 2 will show the solution is
    \[y(t) = \frac{1}{1-t}.\]
    The vertical asymptote illustrates finite-time blow-up.
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 4]
    The differential equation is
    \[y' + \frac{t}{t^2-25}y = \frac{e^t}{t-9}.\]
    The existence and uniqueness theorem requires the coefficients
    \[p(t)=\frac{t}{t^2-25}, \qquad g(t)=\frac{e^t}{t-9}\]
    to be continuous.
    Discontinuities occur when denominators are zero:
    \[t^2-25=0 \Rightarrow t=\pm5, \qquad t-9=0 \Rightarrow t=9.\]
    So the critical points are \(t=-5,\:5,\:9\).
    \begin{itemize}
        \item For \(y(-7)=-2.1\): the nearest discontinuity is \(t=-5\).  
        Largest interval:
        \[(-\infty,-5).\]
        \item For \(y(-1.5)=-3.14\): the nearest discontinuities are \(t=-5\) and \(t=5\).  
        Largest interval:
        \[(-5,5).\]
        \item For \(y(8.5)=6.4\): the nearest discontinuities are \(t=5\) and \(t=9\).  
        Largest interval:
        \[(5,9).\]
        \item For \(y(13)=-0.5\): the nearest discontinuity is \(t=9\).  
        Largest interval:
        \[(9,\infty).\]
    \end{itemize}
\end{solution}
\begin{solution}[LLC]
    We have
    \[f(y) = 3y^{2/3}, \qquad f'(y) = 2y^{-1/3}.\]
    This derivative is not bounded near \(y=0\), so \(f\) is not locally Lipschitz at \(y=0\).
    The function
    \[y_1(t) = 0\]
    is a solution.
    In Chapter 2, solving the separable equation (assuming \(y \neq 0\)) gives another solution
    \[y_2(t) = \begin{cases}t^3, & t \ge 0, \\ 0, & t < 0.\end{cases}\]
    Indeed,
    \[y_2'(t) = 3t^2 = 3(y_2(t))^{2/3}.\]
    Thus, the solution is not unique.
\end{solution}

\section{Useful Links}

\end{document}