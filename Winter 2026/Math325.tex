\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % Required for author affiliations
\usepackage{indentfirst} % Indent first paragraph of sections
\usepackage{amssymb} % For mathematical symbols
\usepackage{amsthm} % For theorem environments
\usepackage{amsmath} % For advanced math typesetting
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\usepackage{pgfplots} % For plots
\pgfplotsset{compat=1.18} % Set compatibility level
\usepackage{tikz} % For drawing shapes
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem*{example}{Example}
\newtheorem{remark}{Remark}
\reversemarginpar
\begin{document}
%------- Title page   -----------
\title{MATH 325: Honours Ordinary Differential Equations}
\author{William Homier}
\affil[1]{McGill University Physics, 3600 Rue University, Montréal, QC H3A 2T8, Canada}
\date{January \(6^{th}\), 2026}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\maketitle

%------- Abstract -----------
\noindent\rule{\textwidth}{0.4pt}
\thispagestyle{empty}
\begin{abstract}

\end{abstract}
\noindent\rule{\textwidth}{0.4pt}
\clearpage

%------- Table of Contents -----------
\thispagestyle{empty}
\hypersetup{
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\tableofcontents
\clearpage

%------- introduction -----------
\setcounter{page}{1}
\section{Introduction}
Jean-Philippe Lessard (Burnside 1119).
Tutorials every wednesday from 9am to 10am, ENGTR 0070, with Eunpyo Bang.
Office hours thursday.
No textbooks.
25\% assignments (2 written assignments 15\%, and 5 webworks 10\%).
25\% Midterm (February 16 - inclass).
50\% Final.
Since its honours you will deal with analysis.

\section{Prerequisite knowledge}
\subsection{Analysis}

\section{Intro, Classification, Theorem of Existence \& Uniqueness}
\subsection{Intro}
\marginpar{January 06, 2026.}
\begin{definition}[Differential Equation]
    A differential equation (DE) is a relation that involves an unknown function and some of its derivatives.
\end{definition}
To better understand what a differential equation is, consider the following example.\\
Imagine a ball of mass \(m\) falling, subject to gravity and air resistance (drag). Denote by \(v(t)\) the velocity of the ball at time \(t\), whereas \(t\) is the independent variable, and \(v\) the dependent variable. Let the downward direction be positive. We know the force of gravity is given by \(F_g = mg\), where \(g\) is the acceleration due to gravity. The drag force is given by \(F_d = -\lambda v\), where \(\lambda\) is the drag coefficient and is \(\lambda \geq 0\). According to Newton's second law \(\sum F = ma\), the net force acting on the ball is equal to its mass times its acceleration
\[m\frac{dv}{dt} = mg - \lambda v.\]
Let \(y(t)\) be the position, meaning \(v(t) = \frac{dy}{dt}\). Then, we can rewrite the above equation as
\[my'' + \lambda y' = mg.\]

Let's analyze another example, population growth (known as the Malthusian growth model).\\
Denote by \(N(t)\) the size of a given population at time t. In an "unconstraint" environment, it is reasonable to assume that the rate of change of the number of individuals is proportional to the number of individuals present. This assumption leads to the following differential equation:
\[\frac{dN}{dt} = rN,\]
where \(r\) is called the growth rate (if \(r > 0\)), and decay rate (if \(r < 0\)). Assume that \(N > 0\). Using the chain rule and assuming that \(N(t)\) satisfies \(N' = rN\)
\[\frac{d}{dt}ln(N(t)) = \frac{dln(N)}{dN} \cdot \frac{dN}{dt} = \frac{1}{N} \cdot N' = r,\]
integrate with respect to t
\[ln(N(t)) = rt + C,\]
where \(C\) is the constant of integration. Exponentiating both sides, we obtain
\[N(t) = e^{ln(N(t))} = e^Ce^{rt} = ke^{rt},\]
where \(\{k > 0 | k \in \mathbb{R}\}\) which could be any positive constant is the initial population size at time \(t = 0\).\\
Assume that an initial population (condition) is given:
\[N(0) = N_0 (fixed),\]
we therefore get that \(k = N_0\), and the unique solution that satisfies the initial condition is
\[N(t) = N_0e^{rt}.\]

The problem with the answer we got in the previous example is that it is not realistic in the long run, how about we consider a carrying capacity\footnote{maximum population size that the environment can sustain indefinitely}. This leads us to another example: Population growth/decay with the carrying capacity of the environment.\\
Now assume that our growth rate depends on the population size \(N(t)\) itself, therefore we get that
\[\frac{dN}{dt} = R(N)N.\]
Denote by K the number of individual that the environment can carry. K is called the carrying capacity of the environment. If \(N < K\), we want growth \((R(N) > 0)\) and if \(N > K\), we want decay \((R(N) < 0)\).\\
\begin{center}
\begin{tikzpicture}[scale=1.1]
    % Axes
    \draw[->] (-0.5,0) -- (6,0) node[right] {$N$};
    \draw[->] (0,-0.5) -- (0,5) node[above] {$R(N)$};

    % Line
    \draw[thick] (0,4) -- (5,0);

    % Intercepts
    \fill (0,4) circle (2pt) node[left] {$r$};
    \fill (5,0) circle (2pt) node[below] {$K$};

\end{tikzpicture}
\end{center}
Let's pick the simplest function \(R(N)\) that satisfies \(R(0) = r, R(K) = 0\) and is linear. We get that
\[R(N) = r(1 - \frac{N}{K}).\]
Therefore, our differential equation becomes
\[\frac{dN}{dt} = r(1 - \frac{N}{K})N = \frac{r}{K}(K - N(t))N(t).\]
This is called the logistic equation.

\begin{definition}[Ordinary Differential Equation]
    An ordinary differential equation (ODE) is a differential equation whose unknown function depends on one independent variable only.
\end{definition}
Example of ODEs:
\begin{itemize}
    \item \(y''(t) + y'(t) + 2y(t) = sin(t)\)
    \item \(N'(t) = rN(t)\)
    \item \(mv'(t) = mg - \lambda v(t)\)
    \item \(y'(x) + 3y(x) = e^x\)
\end{itemize}
\begin{definition}[Partial Differential Equation]
    A partial differential equation (PDE) is a differential equation whose unknown function depends on more than one independent variable. \textbf{Will not be taught in this course.}
\end{definition}
Example of a PDE is the Heat Equation. Let \(u = u(x,t)\), \(\frac{\partial u}{\partial t} = \frac{\partial^2 u}{\partial x^2}\). This PDE denotes the temperature of a body at time t and at position x.

\subsection{Classification}
\marginpar{January 08, 2026.}
\subsubsection{The Order}
\begin{definition}
    The order of an ODE is the order of the highest derivative that appears in the equation.
\end{definition}
\begin{example}
    \(N' = rN\) (first order ODE)
\end{example}
\begin{example}
    \(y''(t) + 2y'(t) = e^t\) (second order ODE)
\end{example}

\subsubsection{Dimension of the State}
\begin{definition}[Scalar ODE]
    A scalar ODE has one unknown function.
\end{definition}
\begin{example}[Scalar ODE]
    \[y'' + y = 0\]
\end{example}
\begin{definition}[System of ODEs]
    A system of ODEs has several unknown functions.
\end{definition}
\begin{example}[System of ODEs]
    \[\begin{cases}
        y_1' = y_2\\
        y_2' = -y_1
    \end{cases}\]
\end{example}
\begin{definition}[Systems of first order ODEs]
    A system of first order ODEs is just a collection of differential equations where all derivatives are first order. It can be written compactly as
    \[y'(t) = f(y(t), t),\]
    where \(y(t)\) is a vector of unknown functions,
    \[y(t) = \begin{pmatrix} y_1(t) \\ \vdots \\ y_n(t) \end{pmatrix},\]
    and \(f\) gives the right-hand sides of the equations.
    Writing this out means
    \[\begin{cases}y_1'(t) = f_1(y_1,\dots,y_n,t) \\ \vdots \\ y_n'(t) = f_n(y_1,\dots,y_n,t).\end{cases}\]
    Any single \(n\)th-order ODE can always be turned into such a system. If
    \[y^{(n)}(t) = G(t, y, y', \dots, y^{(n-1)}),\]
    define new variables
    \[y_1 = y, \quad y_2 = y', \quad \dots, \quad y_n = y^{(n-1)}.\]
    Then each new variable has a first derivative, and the equation becomes
    \[\begin{cases}y_1' = y_2, \\ y_2' = y_3, \\ \vdots \\ y_{n-1}' = y_n, \\ y_n' = G(t, y_1, \dots, y_n).\end{cases}\]
    So one higher-order equation is the same thing as many first order equations.
\end{definition}
\begin{problem}[System of First Order ODEs]
    Rewrite this third order ODE as a first order system:
    \[y''' + 4y' - y = 0\]
\end{problem}
\begin{example}[System of First Order ODEs 2]
    Consider the second-order ODE
    \[y'' + 2y' + y = e^t\]
    \[\Longrightarrow y'' = -2y' - y + e^t.\]
    Define new variables
    \[y_1 = y, \qquad y_2 = y'.\]
    Then the system becomes
    \[\begin{pmatrix}y_1' \\ y_2'\end{pmatrix} = \begin{pmatrix}0 & 1 \\ -1 & -2\end{pmatrix} \begin{pmatrix}y_1 \\ y_2\end{pmatrix} + \begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]
    Let
    \[A = \begin{pmatrix} 0 & 1 \\ -1 & -2 \end{pmatrix},\quad r(t) = \begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]
\end{example}
\begin{example}[Lorenz system]
    \[\begin{aligned}y_1' &= \sigma (y_2-y_1), \\ y_2' &= \rho y_1 - y_2 - y_1y_3, \\ y_3' &= y_1y_2 - \beta y_3,\end{aligned}\]
    where \(\sigma,\rho,\beta\) are parameters. This is a nonlinear first order system in \(\mathbb{R}^3\).
\end{example}

\subsubsection{Linearity}
\begin{definition}[Linearity]
    A linear Ordinary Differential Equation (ODE) is one where the unknown function (\(y\)) and its derivatives (\(y^{\prime },y^{\prime \prime },\dots \)) appear only to the first power, are not multiplied together, and are not part of special functions like \(\sin (y)\) or \(e^{y}\). Essentially, they are "simple" combinations (addition/subtraction) of \(y\) and its derivatives, potentially multiplied by functions of the independent variable (like \(x\) or \(t\)).
\end{definition}
\begin{example}[Linearity]
    Consider the following ODEs:
    \begin{itemize}
        \item Linear: $y' +3y = 0$
        \item Linear: $y'' - 2xy' + y = cos(x)$
        \item Non-linear: $y' + y^2 = 0$
        \item Non-linear: $y'' + sin(y) = 0$
    \end{itemize}
\end{example}

\subsubsection{Autonomy}
\begin{definition}[Autonomy]
    The \(n\)\textsuperscript{th} order ODE \(F(t, y, y', \ldots, y^{(n)}) = 0\) is autonomous if \(F\) does not depend explicitly on t, that is, if it is of the form \(F(y, y', \ldots, y^{(n)}) = 0\). Otherwise, it is non-autonomous.
\end{definition}
\begin{example}[Autonomy]
    Consider the following ODEs:
    \begin{itemize}
        \item Non-autonomous: \(y'' + 2y' + y - e^t = 0\)
        \item Autonomous: \(N'(t) = rN(t)\)
        \item Non-autonomous: \(y'(t) = ty(t)\)
    \end{itemize}
\end{example}
Equivalently, a first-order system \(y' = f(y,t)\) is autonomous if it can be written as \[ y' = f(y). \] Otherwise, it is non-autonomous.
\begin{example}
A classic, simple example of an autonomous first-order system is the linear growth/decay model:
\[\frac{dy}{dt} = ky\]
Here, \(f(y,t) = ky\), which depends only on \(y\) (where \(k\) is a constant), making it autonomous.
Other examples include \(\frac{dy}{dt}=1 - y^{2}\) or exponential growth \(\frac{dy}{dt} = 0.5y\).
\end{example}
\begin{remark}
    The Lorenz system is an example of an autonomous system\footnote{Here “system” means the unknown \(y\) is vector-valued, e.g. \(y \in \mathbb{R}^m\), rather than scalar.}.
\end{remark} 

\subsubsection{Solutions of ODEs}
\begin{definition}[Solutions of ODEs]
    Let \(f: D \times (a,b) \to \mathbb{R}^n\). A solution of \(y'(t) = f(y(t), t)\) on an interval \(J \subset \mathbb{R}\) is a differentiable function \(y: J \to D \subset \mathbb{R}^n\), such that \(y'(t) = f(y(t), t), \forall t \in J\). \(t\) is the independent variable, and \(y = (y_1, \ldots, y_n)\) is the dependent variable.
\end{definition}
\paragraph{1. Explicit Solutions}
\begin{example}[Explicit Solution]
    Consider the following ODE
    \[y' + y = 1.\]
    We can verify that \(y(t) = e^{-t} + 1\), and therefore \(y'(t) = -e^{-t}\), is a solution on \(\mathbb{R}\). Indeed,
    \[y' + y = -e^{-t} + (e^{-t} + 1) = 1.\]
    In this example, \(y = y(t)\) is explicitly given as a function of t (independent variable).
\end{example}
\paragraph{2. Implicit solutions}
\begin{example}[Implicit Solution]
    Consider the ODE
    \[y \frac{dy}{dx} = x.\]
    This is a nonautonomous, nonlinear, first-order scalar ODE. Separating variables gives
    \[y\,dy = x\,dx.\]
    Integrating,
    \[\frac{1}{2}y^2 = \frac{1}{2}x^2 + C,\]
    or equivalently, the implicit solution
    \[x^2 - y^2 = C, \quad C \in \mathbb{R}.\]
    To verify, differentiate implicitly:
    \[\frac{d}{dx}(x^2 - y^2) = 0 \;\Longrightarrow\; 2x - 2y\frac{dy}{dx} = 0\;\Longrightarrow\; y\frac{dy}{dx} = x.\]
\end{example}

\subsection{Initial Value Problems}
\marginpar{January 13, 2026}
\begin{definition}[Initial Value Problem (IVP)]
    An initial value problem (IVP) is a system of ODEs with an initial condition.
\end{definition}
\begin{example}[IVP]
    Consider the following ODE 
    \[y' = f(y,t), \quad f : D \times (a,b) \to \mathbb{R}^n.\]
    Let \(t_0 \in (a,b)\). An initial condition is
    \[y(t_0) = y_0 \in \mathbb{R}^n.\]
    An initial value problem (IVP) is
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}.\]
\end{example}


\subsection{Existence and Uniqueness Theorem}

\subsubsection{Lipschitz continuity}
\begin{definition}[Lipschitz continuity]
    Let \(D \subseteq \mathbb{R}^n\) and let \(||\cdot||\) be a norm on \(\mathbb{R}^n\).
    A function \(f : D \to \mathbb{R}^n\) is Lipschitz continuous if there exists \(L \ge 0\) such that
    \[z||f(y_1) - f(y_2)|| \le L||y_1 - y_2|| \quad \forall y_1,y_2 \in D, z \in \mathbb{R}.\]
    The smallest such \(L\) is called the Lipschitz constant and is denoted \(Lip(f)\).
\end{definition}
\begin{example}
    Let \(f(y) = 4y - 5\), \(D = \mathbb{R}\), and \(||\cdot|| = |\cdot|\).
    Then
    \[|f(y_1) - f(y_2)| = |4y_1 - 4y_2| = 4|y_1 - y_2|.\]
    So \(f\) is Lipschitz with \(Lip(f) = 4\).
\end{example}
\begin{example}
    Let
    \[f(y) = \frac{1}{y - 1}, \quad f'(y) = -\frac{1}{(y - 1)^2}, \quad D = (1, +\infty).\]
    This function is not Lipschitz continuous on \(D\) since as \(y \to 1^+\), the derivative \(f'(y)\) approaches \(-\infty\), and so the ratio \(\frac{|f(y_2) - f(y_1)|}{|y_2 - y_1|}\) approaches \(\infty\) for any \(y_1,y_2 \in D\). Therefore, there is no Lipschitz constant for \(f\) on \(D\).\\
    To make this function Lipschitz continuous, we fix \(\delta > 1\) and define \(D_\delta = (\delta, +\infty)\). For \(y_1,y_2 \in D_\delta\), by the Mean Value Theorem, there exists \(z \in (y_1,y_2)\) such that
    \[f(y_2) - f(y_1) = f'(z)(y_2 - y_1).\]
    So
    \[|f(y_2) - f(y_1)| \le \frac{1}{(z-1)^2}|y_2 - y_1|\le \frac{1}{(\delta-1)^2}|y_2 - y_1|.\]
    Thus \(f\) is Lipschitz on \(D_\delta\) with
    \[Lip(f) = \frac{1}{(\delta - 1)^2}.\]
\end{example}

\subsubsection{Local Lipschitz continuity}
\begin{definition}[Locally Lipschitz]
    Let \(D \subseteq \mathbb{R}^n\) be open. A function \(f : D \to \mathbb{R}^n\) is called locally Lipschitz if, around every point in \(D\), there is some neighborhood where \(f\) is Lipschitz.
    Equivalently, for every compact set \(K \subset D\), there exists a constant \(L > 0\) such that
    \[\|f(y_1) - f(y_2)\| \le L \|y_1 - y_2\| \quad \text{for all } y_1, y_2 \in K.\]
\end{definition}
\begin{remark}
    Lipschitz continuity and local Lipschitz continuity are not the same.
    \[\text{Lipschitz} \;\Rightarrow\; \text{locally Lipschitz}, \qquad \text{but not conversely.}\]
\end{remark}
\begin{example}[Locally Lipschitz but not Lipschitz]
    \[f(y) = y^2 \in \mathbb{R}.\]
    Since \(f'(y)=2y\) is unbounded on \(\mathbb{R}\), no single constant works on the whole domain, so \(f\) is not Lipschitz.  
    However, on any bounded set \(K=[-M,M]\), \(|f'(y)| \le 2M\), so \(f\) is locally Lipschitz.
\end{example}
\begin{example}[Lipschitz (hence locally Lipschitz)]
    \[f(y) = \sin y.\]
    Since \(|f'(y)| = |\cos y| \leq 1\), $\forall y$, $f$ is Lipschitz on \(\mathbb{R}\).
\end{example}
\begin{example}[Continuous but not locally Lipschitz]
    \[f(y) = \sqrt{|y|}.\]
    The derivative is unbounded near \(y = 0\), so no Lipschitz constant exists even locally. Hence \(f\) is not locally Lipschitz.
\end{example}

\subsubsection{Existence and Uniqueness Theorem}
\begin{theorem}[Existence and Uniqueness]
    Let \(D \subseteq \mathbb{R}^n\) be open and let \((a,b)\) be an open interval containing \(t_0\). Consider the IVP
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}\]
    Assume \(f : D \times (a,b) \to \mathbb{R}^n\) is continuous and locally Lipschitz in \(y\).\footnote{\(D \times (a,b)\) denotes the set of all pairs \((y,t)\) with \(y \in D \subset \mathbb{R}^n\) and \(t \in (a,b)\), i.e., all allowed state-time inputs of \(f\).}
    If \(y_0 \in D\), then there exists an open interval \(J\) containing \(t_0\) on which a solution exists.
    Moreover, this solution is unique on \(J\).
\end{theorem}
\begin{problem}[Existence and Uniqueness Theorem]
    Consider the IVP
    \[\begin{cases}y' = \sqrt{1+y^2}+t^2 \\ y(1) = 0.\end{cases}\]
    \begin{enumerate}
        \item Identify $f(y,t)$.
        \item Decide whether the hypotheses of the Existence and Uniqueness Theorem are satisfied.
        \item State clearly what the theorem guarantees about solutions near $t = 1$.
    \end{enumerate}
    Do not solve the ODE. Simply analyze it.
\end{problem}

\subsubsection{Integral form of solutions}
\begin{lemma}
    A function \(y\) solves the IVP if and only if
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
\end{lemma}
\begin{proof}
    If \(y' = f(y,t)\) and \(y(t_0) = y_0\), then by the Fundamental Theorem of Calculus,
    \[y(t) - y(t_0) = \int_{t_0}^{t} f(y(s),s)ds,\]
    so
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
    Conversely, differentiating the right-hand side gives
    \[y'(t) = f(y(t),t), \quad y(t_0) = y_0.\]
\end{proof}

\subsubsection{Picard operator}%to organize
\paragraph{1. Setup and notation}
Let \((y_0,t_0) \in D \times (a,b)\). Since this set is open, there exist \(\alpha,\delta > 0\) such that
\[D_{\alpha,\delta} = \{(y,t) : ||y - y_0|| \le \alpha, |t - t_0| \le \delta\}
\subset D \times (a,b).\]

Define
\[M_{\alpha,\delta} = \sup_{(y,t) \in D_{\alpha,\delta}} ||f(y,t)|| < +\infty.\footnote{\(sup\) means the largest value in a set of numbers.}\]

Let
\[\epsilon = \min\left(\delta, \frac{\alpha}{M_{\alpha,\delta}}\right), \quad J = (t_0 - \epsilon, t_0 + \epsilon).\]

\paragraph{2. Definition of the Picard operator}
\begin{lemma}[Picard operator]
    For any function \(y\) such that \(y(t_0)=y_0\) and \((y(t),t) \in D_{\alpha,\delta}\) for all \(t \in J\), define
    \[T(y)(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
    Then \(T(y)(t_0)=y_0\) and \((T(y)(t),t) \in D_{\alpha,\delta}\) for all \(t \in J\).
\end{lemma}

\begin{proof}
    \[T(y)(t_0) = y_0.\]
    For \(t \in J\),
    \[||T(y)(t) - y_0|| \leq \int_{t_0}^{t} ||f(y(s),s)||ds \leq M_{\alpha,\delta}|t - t_0| \leq M_{\alpha,\delta}\epsilon \leq \alpha.\]
    So \((T(y)(t),t) \in D_{\alpha,\delta}\).
\end{proof}

\marginpar{January 15, 2026}
\paragraph{3. Invariant properties}
\begin{lemma}
    If $y : J \to \mathbb{R}^n$ satisfies 
    \begin{enumerate}
        \item $y(t_0) = y_0$,
        \item $(y(t),t) \in D_{\alpha,\delta}$ for all $t \in J$,
    \end{enumerate}
    then $T(y) : J \to \mathbb{R}^n$ satisfies the same properties.
\end{lemma}

\paragraph{3. Picard iterations}
Define $y_0(t) = y_0$ (constant function), which clearly satisfies (1) and (2).

For $k \geq 1$, define
\[y_k(t) = T(y_{k-1})(t) = y_0 + \int_{t_0}^{t} f(y_{k-1}(s),s)ds.\]

\paragraph{4. Existence}
The Picard iterations converge uniformly to a function $y : J \to \mathbb{R}^n$ which satisfies (1) and (2), and is a solution of the IVP.
\begin{proof}
    Pick $t \in [t_0, t_0 + \epsilon]$ (the proof is similar for $t \in [t_0 - \epsilon, t_0]$).
    The goal is to show that $\{y_k(t)\}_{k=0}^{\infty}$ is a Cauchy sequence\footnote{Cauchy sequence is a sequence that has a limit in a metric space $\mathbb{R}^n$.} in $\mathbb{R}^n$.
    We prove by induction that
    \[(**)\quad ||y_m(t) - y_{m - 1}(t)|| \leq L^{m - 1}M_{\alpha,\delta} \frac{(t - t_0)^m}{m!},\:\forall m \geq 1.\]
    \textbf{Base case \(m=1\):}
    \[||y_1(t) - y_0(t)|| = \left\| \int_{t_0}^{t} f(y_0,s)ds \right\| \leq \int_{t_0}^{t}||f(y_0,s)||ds \leq M_{\alpha,\delta}|t - t_0| \leq \alpha.\]
    \textbf{Induction step:}
    \[||y_{m + 1}(t) - y_m(t)|| \leq \int_{t_0}^{t}||f(y_m(s),s) - f(y_{m-1}(s),s)||ds.\]
    Since $D_{\alpha,\delta}$ is compact and $f$ is Lipschitz on $D_{\alpha,\delta}$, there exists $L$ such that
    \[||f(x,t) - f(y,t)|| \leq L||x - y||.\]
    Thus,
    \[\leq L \int_{t_0}^{t}||y_m(s) - y_{m - 1}(s)||ds.\]
    Using (**),
    \[\leq L^m M_{\alpha,\delta} \frac{1}{(m-1)!} \int_{t_0}^{t}(s - t_0)^{m-1}ds
    = L^m M_{\alpha,\delta} \frac{(t - t_0)^m}{m!}.\]
    Hence (**) holds.
    In particular, for all $\rho \geq 1$,
    \[||y_\rho(t) - y_{\rho-1}(t)|| \leq M_{\alpha,\delta} \frac{(L(t - t_0))^{\rho}}{(\rho)!} < \frac{M_{\alpha,\delta}}{L}\frac{(L\epsilon)^\rho}{\rho!}.\]
    Let $m,p \geq 1$:
    \[||y_{m+p}(t) - y_{m+1}(t)|| \leq \sum_{k=1}^{p-1}||y_{m+k+1}(t) - y_{m+k}(t)||.\]
    So,
    \[< \frac{M_{\alpha,\delta}}{L}\sum_{j=m+2}^{m+p}\frac{(L\epsilon)^j}{j!}.\]
    Since \(e^{L\epsilon} = \sum_{j=0}^{\infty}\frac{(L\epsilon)^j}{j!}\) converges,
    \[\to_{m,p \to +\infty} 0.\]
    Thus $\{y_k(t)\}$ is Cauchy and converges to $y(t)$.
    Taking limits in the iteration,
    \[y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
\end{proof}\footnote{Evaluated on compacted cylinder, understanding epsilon, the L Lipschitz constant coming from somewhere, not on the analysis background such as the Banach fixed point theorem.}
\paragraph{Uniqueness}
\begin{proof}
    Assume $y(t)$ and $z(t)$ solve the IVP. Then
    \[||y(t) - z(t)|| \leq \int_{t_0}^{t}||f(y(s),s) - f(z(s),s)||ds \leq L \int_{t_0}^{t}||y(s) - z(s)||ds.\]
    Define \(g(t) = \int_{t_0}^{t}||y(s) - z(s)||ds\). Then
    \[g'(t) \leq Lg(t).\]
    So,
    \[\frac{d}{dt}(e^{-L(t-t_0)}g(t)) \leq 0.\]
    Thus \(e^{-L(t-t_0)}g(t)\) is decreasing and
    \[0 \leq g(t) \leq g(t_0) = 0.\]
    Hence \(g(t)=0\) and \(y(t)=z(t)\).
\end{proof}

\marginpar{January 20, 2026}
\paragraph{5. Examples}
\begin{remark}[How \(\varepsilon\) is chosen in the theorem]
    To apply the Existence and Uniqueness Theorem, we choose \(\varepsilon\) so that the solution remains inside the domain \(D \times (a,b)\).
    Consider the IVP
    \[\begin{cases}y' = f(y,t), \\ y(t_0) = y_0.\end{cases}\]
    The theorem guarantees that there exists a unique solution 
    \[y : (t_0-\varepsilon,\, t_0+\varepsilon) \to \mathbb{R}^n.\]
    Choose \(\alpha > 0\) and \(\delta > 0\) such that
    \[D_{\alpha,\delta} := \{(y,t) : \|y-y_0\| \le \alpha,\ |t-t_0| \le \delta\} \subset D \times (a,b).\]
    Since \(f\) is continuous, define
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} \|f(y,t)\| < \infty.\]
    Then one may take
    \[\varepsilon = \min\!\left(\delta,\, \frac{\alpha}{M_{\alpha,\delta}}\right).\]
\end{remark}\footnote{In this course, you are only expected to determine \(\varepsilon\) from the theorem, and not to justify the construction further.}

\begin{problem}[Existence and Uniqueness Theorem]
    Consider the initial value problem
    \[\begin{cases}y' = y + 1, \\ y(0) = 1,\end{cases} \qquad \text{with } t_0 = 0,\; y_0 = 1.\]
    Use the Existence and Uniqueness Theorem to determine an interval on which a unique solution is guaranteed to exist. Compute \(\varepsilon\) explicitly using the theorem.
\end{problem}
\begin{problem}[Existence and Uniqueness Theorem 2]
    Consider the initial value problem
    \[\begin{cases}y' = y + 1, \\ y(0) = 1,\end{cases} \qquad t_0 = 0,\; y_0 = 1.\]
    Use the Existence and Uniqueness Theorem to determine an interval on which a unique solution is guaranteed to exist.
\end{problem}
\begin{problem}[Existence and Uniqueness Theorem 3]
    Consider the initial value problem
    \[\begin{cases}y' = y^2, \\ y(0) = 1.\end{cases}\]
    Use the Existence and Uniqueness Theorem to estimate an interval of existence.
\end{problem}
\begin{problem}[Existence and Uniqueness Theorem 4]
    Consider the initial value problem
    \[\begin{cases}y' = t^2 + y^2, \\ y(0) = 0.\end{cases}\]
    Use the Existence and Uniqueness Theorem to estimate \(\varepsilon\).
\end{problem}
\begin{problem}[LLC]
    Consider the initial value problem
    \[\begin{cases}y' = 3y^{2/3}, \\ y(0) = 0.\end{cases}\]
    Investigate existence and uniqueness.
\end{problem}

\section{First-Order Scalar Equation}
\marginpar{January 22, 2026}
\[y' = f(y,t) \in \mathbb{R} \quad (n=1)\:One\:equation\]
\subsection{First order linear equations}
\[a_0(t)y' + a_1(t)y = g(t)\]
where $a_0,a_1,g$ are functions of $t$.
Dividing by $a_0$ leads to
\[y' + p(t)y = q(t)\]
where $p(t) = \frac{a_1(t)}{a_0(t)}$ and $q(t) = \frac{g(t)}{a_0(t)}$
Multiply the ODE by an integrating factor $\mu(t)$ (to be determined).
\[\mu(t)y' + \mu(t)p(t)y = \mu(t)q(t)\]
Requirement in $\mu$ is that $\mu(t)y' + \mu(t)p(t)y = \frac{d}{dt}(\mu(t)y(t))$ (*) holds.
In this case, integrate with respect to $t$.
\[\mu(t)y(t) = \int\mu(t)q(t)dt + C,\]
where $C \in \mathbb{R}$ is a constant of integration. We therefore get
\[y(t) = \frac{1}{\mu(t)}\int\mu(t)q(t)dt + C.\]
Let us find $\mu$ so that (*) holds.
\[\frac{d}{dt}(\mu(t)y(t)) = \mu(t)y' + \mu(t)p(t)y = \mu'(t)y(t) + \mu(t)y',\]
this implies that
\[\mu'(t)y(t) = \mu(t)p(t)y(t).\]
It is sufficient to have that
\[\mu'(t) = \mu(t)p(t).\]
Now we can solve for $p(t)$
\[\frac{d}{dt}ln|\mu(t)| = \frac{\mu'(t)}{\mu(t)} = p(t),\]
now integrate with respect to $t$
\[ln|\mu(t)| = \int p(t)dt + C,\]
where $C \in \mathbb{R}$ is a constant of integration, and now we exponentiate
\[|\mu(t)| = e^{\int p(t)dt + C_1} = e^{C_1}e^{\int p(t)dt}\]
\[\mu(t) = (\pm e^{C_1})e^{\int p(t)dt}.\]
We can therefore say choose $\mu(t) = e^{\int p(t)dt}$ (integrating factor).
This is the general solution of the first order linear equation.

\begin{example}
    \[y' - 2y = 3e^t\]
    \[p(t) = -2, q(t) = 3e^t\]
    \[\mu(t) = e^{\int -2dt} = e^{-2t}\]
    \[e^{-2t}(y' - 2y) = e^{-2t}3e^t = 3e^{-t}\]
    \[e^{-2t}(y' - 2y) = \frac{d}{dt}(e^{-2t}y(t))\]
    integrate with respect to $t$
    \[e^{-2t}y(t) = \int e^{-2t}3e^{-t}dt + C = -3e^{-t} + C\]
    Using this explicit solution
    \[y(t) = \frac{1}{\mu(t)}[\int \mu(t)q(t)dt + C]\]
    we can find the general solution
    \[y(t) = e^{2t}(-3e^{-t} + C) = -3e^t + Ce^{2t}\]
    If $y(0) = 1$, then $y(0) = -3 + C = 1$, so $C = 4$.
    \[y(t) = -3e^t + 4e^{2t}\]
\end{example}

\begin{theorem}[Existence \& Uniqueness for Linear Equations]
    \[\begin{cases}
        y' + p(t)y = q(t)\\
        y(t_0) = y_0
    \end{cases}\]
    Assume that the functions $p$ and $q$ are continuous on some interval $(a,b)$, where $(-\infty \leq a < b \leq \infty)$, and assume that $t_0 \in (a,b)$.
    Then there is a unique solution $y(t)$ to the IVP and $J_{max} = (a,b)$.
\end{theorem}
\begin{example}
    \[\begin{cases}
        y' + \frac{1}{t - 1}y = \frac{1}{cos(t)}\\
        y(0) = 1
    \end{cases}\]
    \[J_{max} = (-\frac{\pi}{2}, 1)\]
\end{example}
\begin{example}[Falling Object]
    Imagine a falling object, subject to gravity and air drag.
    \[v'(t) + \frac{\gamma}{m}v(t) = g\]
    This is a first order linear equation. The integrating factor is
    \[\mu(t) = e^{\int \frac{\gamma}{m}dt} = e^{\frac{\gamma}{m}t}\]
    \[\frac{d}{dt}(e^{\frac{\gamma}{m}t}v(t)) = e^{\frac{\gamma t}{m}}(v' + \frac{\gamma}{m}v) = e^{\frac{\gamma t}{m}g}\]
    We get
    \[e^{\frac{\gamma t}{m}}v(t) = g\int e^{\frac{\gamma t}{m}}dt + C = \frac{gm}{\gamma}e^{\frac{\gamma t}{m}} + C\]
    \[v(t) = \frac{gm}{\gamma} + Ce^{-\frac{\gamma t}{m}}\]
    where $C \in \mathbb{R}$.
    \[v(0) = \frac{gm}{\gamma} = \frac{gm}{\gamma} + C\]
    Which implies
    \[C = 0\]
    \[y(t) = \frac{gm}{\gamma}\]
\end{example}
\begin{example}[Mixing Problems]
    Imagine a tank, at $t = 0$, the tank contains 100 litres of brine water, in which $y_0$ grams of salt is dissolved. You pour in 50g of salt per litre in the tank, and there is a flow in of water at a rate of $R$ litres per second. Water is flowing out of the tank with a rate of $R$ litres per seconds.
    Denote by $y(t)$ the quantity of salt in grams in the tank at time $t$ in seconds.
    $\frac{dy}{dt}$ is the rate in - the rate out.
    \[\frac{dy}{dt} = 50 [\frac{grams}{litres}] \times R [\frac{litres}{seconds}] - \frac{y(t)}{100} [\frac{g\:of\:salt}{litres}]\]
    We get now that
    \[y' = 50R - \frac{R}{100}y\]
    Rewrite it as
    \[y' + \frac{R}{100}y = 50R\]
    and
    \[y(0) = y_0\]
    Now we can solve this IVP.
    \[\mu(t) = e^{\frac{Rt}{100}}\]
    \[\frac{d}{dt}(e^{\frac{Rt}{100}}y(t)) = e^{\frac{Rt}{100}} \cdot 50R\]
    \[e^{\frac{Rt}{100}}y(t) = 50R \frac{100}{R} e^{\frac{Rt}{100}} + C = 5000e^{\frac{Rt}{100}} + C\]
    \[y(t) = 5000 + Ce^{-\frac{Rt}{100}}\]
    \[y(0) = 5000 + C = y_0\]
    \[C = y_0 - 5000\]
    \[y(t) = 5000 + (y_0 - 5000)e^{-\frac{Rt}{100}}\]

    Draw graph with threshold 5000, and two exponentials approaching the threshold from the top and below.
\end{example}

\subsection{Separable Equations}
\marginpar{January 27, 2026}
Notes:
\[\frac{dy}{dx} = f(x,y) = f_1(x)f_2(y) \to\]
\[\int \frac{dy}{f_2(y)} = \int f_1(x)dx + C\]
We call the ODE separable if $f(x,y) = f_1(x)f_2(y)$
\begin{example}
    \[\frac{dy}{dx} = -\frac{x}{y}\]
    \[f_1(x) = x,\quad f_2(y) = -\frac{1}{y}\]
\end{example}
\begin{example}
    This following ODE is not separable
    \[\frac{dy}{dx} = x^2 + y^2\]
\end{example}
\[\frac{dy}{dx} = f_1(x)f_2(y) \leftrightarrow -f_1(x) + \frac{1}{f_2(y)}\frac{dy}{dx} = 0 \leftrightarrow M(x) + N(y)\frac{dy}{dx} = 0\]
Consider $H_1(x)$ and $H_2(y)$ antiderivatives of $M(x)$ and $N(y)$, respectively.
From the Chain Rule $\frac{d}{dx}H_2(y(x)) = H_2'(y)\frac{dy}{dx}$
\[H_1'(x) + H_2'(y)\frac{dy}{dx} = 0 \leftrightarrow \frac{d}{dx}(H_1(x) + H_2(y(x))) = 0\]
\[\rightarrow H_1(x) +H_2(y(x)) = C\] is an implicit general solution.
\[\leftrightarrow \int M(x)dx + \int N(y)dy = C\]
\[\leftrightarrow \int -f_1(x)dx + \int \frac{dy}{f_2(y)} = C\]
\[\leftrightarrow \int \frac{dy}{f_2(y)} = \int f_1(x)dx + C, \quad C \in \mathbb{R}\]

\begin{example}
    \[\frac{dy}{dx} = \frac{x^2}{1 - y^2}\]
    \[\int (1 - y^2)dy = \int x^2dx + C\]
    \[\Psi(x,y) = y - \frac{y^3}{3} - \frac{x^3}{3} = C\]
\end{example}

\begin{example}[Logistic Equation]
    \[\frac{dN}{dt} = rN(1 - \frac{N}{K})\]
    This ODE is non-linear but is separable.
    \[\longrightarrow \int\frac{KdN}{N(N - K)} = -\int rdt + C\]
    \[\int \frac{K}{N(N - K)} = -rt + C\]
    Using the method of partial fractions.
    \[\frac{K}{N(N - K)} = \frac{A}{N} + \frac{B}{N - K} = \frac{A(N-K) + BN}{N(N-K)}\]
    \[K = (A+B)N - AK \longrightarrow A+B = 0, \quad -AK = K, \quad \therefore A = -1, B = 1\]
    \[\longrightarrow -ln|N| + ln|N-K| = -rt +C\]
    \[\longrightarrow ln|\frac{N-K}{N}| = -rt + C\]
    \[\longrightarrow |\frac{N-K}{N}| = e^{-rt + C} = C_1 e^{-rt}\]
    \[\longrightarrow \frac{N-K}{N} = (\pm C_1)e^{-rt} = C_2 e^{-rt}\]
    At $t=0$, $N(0) = N_0$
    \[N-K = C_2Ne^{-rt}\]
    \[\longrightarrow N(t) = \frac{K}{1-C_2e^{-rt}} = \frac{K}{\frac{N0}{N0} - (\frac{N_0-K}{N_0})e^{-rt}}\]
    \[\longrightarrow N(t) = \frac{KN_0}{N_0 + (K-N_0)e^{-rt}}\]
    If $N_0 = 0$, then $N(t) = 0, \forall t$. If $N_0 = K$, then $N(t) = K, \forall t$. If $N_0 \in (0,K)$, $N_0 \leq N_0 + (K-N_0)e^{-rt}$ as $N_0$ decreases $\longrightarrow N(t) \to \frac{KN_0}{N_0} = K$. If $N_0 > K$, verify that $N(t) \to K$ as $t \to +\infty$.
\end{example}

Denote $\Psi(x,y) = H_1(x) + H_2(y)$ is the potential function.
Note that the general solution $\Psi(x,y) = C$ is given by the level curves of the potential function.
\begin{definition}[Integral Curve]
    An integral curve is a level curve of the potential function.
\end{definition}
\begin{example}
    \[\frac{dy}{dx} = -\frac{x}{y} \to \int ydy = \int -xdx + C\]
    \[\longrightarrow \Psi(x,y) = \frac{y^2}{2} + \frac{x^2}{2} = C, \quad (C \geq 0)\] this is a general implicit solution.
\end{example}
\begin{example}
    \[x^2 + y^2 = (\sqrt{2C})^2\]
    Assume that $y(0) = 2$ and $x_0 = 0$
    \[\frac{x_0^2}{2} + \frac{y_0^2}{2} = 2 = C\]
    The unique integral curve that contains the initial condition $(x_0, y_0) = (0,2)$ is
    \[\frac{x^2}{2} + \frac{y^2}{2} = 2\]
    \[\longrightarrow y^2 = 4 - x^2\]
    \[\longrightarrow y(x) = \pm \sqrt{4 - x^2}\]
    \[\longrightarrow y(x) = \sqrt{4 - x^2}, \quad J_max = (-2,2)\]
\end{example}
An integral curve is a geometric object that contains (possibly many) solutions. The unique integral curve that contains the initial condition $y(x_0) = y_0$ is defined by
\[\Gamma_{(x_0,y_0)} = \{(x,y) | \Psi(x,y) = \Psi(x_0,y_0) = C(x_0,y_0)\}\] 
\[\frac{d}{dx}[\int f_1(x)dx + \int \frac{dy}{f_2(y)} = C]\]

\begin{definition}[Separable Equations]
    An equation of the form
    \[\begin{cases}
        y' + p(t)y = q(t)\\
        y(0) = y_0
    \end{cases}\]
    where $p(t)$ and $q(t)$ are functions of $t$ is called a separable equation.
\end{definition}


\section{Systems of Linear Equations}
\section{Second and Higher-Order Scalar Linear Equations}
\section{Stability, Phase Portraits and Orbits}
\section{Laplace Transform}
\section{Power Series Solutions and Numerical Methods}


\section{Solutions}
\begin{solution}[System of First Order ODEs]
    Define new variables:
    \[y_1 = y, \quad y_2 = y', \quad y_3 = y''.\]
    Now we can rewrite the third order ODE into a first order system:
    \[\begin{cases}
        y_1' = y_2\\
        y_2' = y_3\\
        y_3' = -4y_2 + y_1
    \end{cases}\]
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem]
    We have the IVP
    \[\begin{cases}y' = \sqrt{1 + y^2} + t^2 \\ y(1) = 0\end{cases}\]
    \begin{enumerate}
        \item The function is simply $f(y,t) = \sqrt{1 + y^2} + t^2$.
        \item Check the hypotheses:
        \begin{itemize}
            \item $f$ is continuous $\forall y \in \mathbb{R},\:t \in \mathbb{R}$ because square root of $1 + y^2$ and $t^2$ are continuous everywhere.
            \item Check local Lipschitz in $y$: $\frac{\partial f}{\partial y} = \frac{y}{\sqrt{1 + y^2}}$, which is continuous $\forall y$. Therefore $f$ is locally Lipschitz in $y$. So the hypotheses of E \& U th. are satisfied in $\mathbb{R} \times \mathbb{R}$.
        \end{itemize}
        \item Since $f$ is continuous and locally Lipschitz in $y$, and since $y(1) = 0$ with $0 \in \mathbb{R}$, the Existence and Uniqueness Theorem guarantees that there exists an open interval $J$ containing $t = 1$ on which a solution exists, and this solution is unique on $J$.
    \end{enumerate}
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem]
    We write
    \[f(y,t) = y + 1.\]
    Since \(f\) is a \(C^1\) function, it is locally Lipschitz in \(y\), and the Existence and Uniqueness Theorem applies.
    The domain is
    \[f : D \to \mathbb{R}, \qquad D = \mathbb{R} = (-\infty, +\infty).\]
    Since \(f\) is defined everywhere, there are no constraints on \(\alpha\) and \(\delta\).
    Define
    \[D_{\alpha,\delta} = \{(y,t) : \|y-1\| \le \alpha,\ |t-0| \le \delta\} = [1-\alpha,\,1+\alpha] \times [-\delta,\,\delta] \subset \mathbb{R} \times \mathbb{R}.\]
    Then
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} \|f(y,t)\| = \sup_{y\in[1-\alpha,\,1+\alpha]} |y+1| = 2+\alpha.\]
    The theorem allows us to take
    \[\varepsilon = \min\!\left(\delta,\, \frac{\alpha}{2+\alpha}\right).\]
    Pick \(\alpha = 1\), \(\delta = 1\):
    \[\varepsilon = \min\!\left(1,\, \frac{1}{3}\right) = \frac{1}{3}.\]
    Therefore, there exists a unique solution on \(\left(-\frac{1}{3},\, \frac{1}{3}\right)\).
    Pick \(\alpha = 3\), \(\delta = 2\):
    \[\varepsilon = \min\!\left(2,\, \frac{3}{5}\right) = \frac{3}{5}.\]
    Therefore, there exists a unique solution on \(\left(-\frac{3}{5},\, \frac{3}{5}\right)\).
    Since there are no constraints on \(\alpha\) and \(\delta\), we can make \(\varepsilon\) as large as we want. Hence, the solution exists and is unique on \(\mathbb{R}\).
    The “maximal” time interval guaranteed by the Existence and Uniqueness Theorem is
    \[J = (-1,1).\]
    In Chapter 2 we will see that the explicit solution is
    \[y(t) = 2e^t - 1.\]
    In fact, the maximal interval on which the solution is defined is
    \[J_{\max} = \mathbb{R}.\]
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 2]
    We write
    \[f(y,t) = y + 1.\]
    Since \(f\) is a \(C^1\) function, it is locally Lipschitz in \(y\), and the theorem applies.
    The domain is
    \[f : D \to \mathbb{R}, \qquad D = \mathbb{R} = (-\infty, +\infty).\]
    Since \(f\) is defined everywhere, there are no constraints on \(\alpha\) and \(\delta\).
    Define
    \[D_{\alpha,\delta} = \{(y,t) : \|y-1\| \le \alpha,\ |t-0| \le \delta\} = [1-\alpha,\,1+\alpha] \times [-\delta,\,\delta] \subset \mathbb{R} \times \mathbb{R}.\]
    Then
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} |f(y,t)| = \sup_{y\in[1-\alpha,\,1+\alpha]} |y+1| = 2+\alpha.\]
    Hence,
    \[\varepsilon = \min\!\left(\delta,\, \frac{\alpha}{2+\alpha}\right).\]
    Pick \(\alpha = 1\), \(\delta = 1\):
    \[\varepsilon = \frac{1}{3},\]
    so a unique solution exists on \(\left(-\frac{1}{3},\,\frac{1}{3}\right)\).
    Pick \(\alpha = 3\), \(\delta = 2\):
    \[\varepsilon = \frac{3}{5},\]
    so a unique solution exists on \(\left(-\frac{3}{5},\,\frac{3}{5}\right)\).
    Since there are no constraints on \(\alpha,\delta\), \(\varepsilon\) can be made arbitrarily large. Therefore, the solution exists and is unique on \(\mathbb{R}\).
    The “maximal” interval guaranteed by the theorem is \(J = (-1,1)\).
    In Chapter 2, we will see the explicit solution is
    \[y(t) = 2e^t - 1.\]
    In fact, the maximal interval of existence is
    \[J_{\max} = \mathbb{R}.\]
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 3]
    Since \(f(y)=y^2\) is \(C^1\), it is locally Lipschitz, so a solution exists and is unique locally.
    On \(D_{\alpha,\delta} = [1-\alpha,\,1+\alpha] \times [-\delta,\,\delta]\),
    \[M_{\alpha,\delta} = \sup_{y \in [1-\alpha,\,1+\alpha]} |y^2| = (1+\alpha)^2.\]
    Thus,
    \[\varepsilon = \min\!\left(\delta,\, \frac{\alpha}{(1+\alpha)^2}\right).\]
    Define
    \[h(\alpha) = \frac{\alpha}{(1+\alpha)^2}, \qquad h'(\alpha) = \frac{1-\alpha}{(1+\alpha)^3}.\]
    Setting \(h'(\alpha)=0\) gives \(\alpha=1\).
    Pick \(\alpha=1\), \(\delta = 104073\):
    \[\varepsilon = \frac{1}{4}.\]
    Therefore, there exists a unique solution
    \[y : \left(-\frac14,\,\frac14\right) \to \mathbb{R}.\]
    Chapter 2 will show the solution is
    \[y(t) = \frac{1}{1-t}.\]
    The vertical asymptote illustrates finite-time blow-up.
\end{solution}
\begin{solution}[Existence and Uniqueness Theorem 4]
    On
    \[D_{\alpha,\delta} = [-\alpha,\alpha] \times [-\delta,\delta],\]
    we have
    \[M_{\alpha,\delta} = \sup_{(y,t)\in D_{\alpha,\delta}} |t^2 + y^2| = \delta^2 + \alpha^2.\]
    Hence,
    \[\varepsilon = \min\!\left(\delta,\, \frac{\alpha}{\delta^2+\alpha^2}\right).\]
    Pick \(\alpha = 10^{12}\), \(\delta = 1\):
    \[\varepsilon = \min\!\left(1,\, \frac{10^{12}}{1+10^{24}}\right) = \frac{10^{12}}{1+10^{24}} \approx 0.\]
\end{solution}
\begin{solution}[LLC]
    We have
    \[f(y) = 3y^{2/3}, \qquad f'(y) = 2y^{-1/3}.\]
    This derivative is not bounded near \(y=0\), so \(f\) is not locally Lipschitz at \(y=0\).
    The function
    \[y_1(t) = 0\]
    is a solution.
    In Chapter 2, solving the separable equation (assuming \(y \neq 0\)) gives another solution
    \[y_2(t) = \begin{cases}t^3, & t \ge 0, \\ 0, & t < 0.\end{cases}\]
    Indeed,
    \[y_2'(t) = 3t^2 = 3(y_2(t))^{2/3}.\]
    Thus, the solution is not unique.
\end{solution}

\section{Appendix}

\section{Useful Links}

\end{document}