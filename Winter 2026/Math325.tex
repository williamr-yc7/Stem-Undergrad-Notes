\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % Required for author affiliations
\usepackage{indentfirst} % Indent first paragraph of sections
\usepackage{amssymb} % For mathematical symbols
\usepackage{amsthm} % For theorem environments
\usepackage{amsmath} % For advanced math typesetting
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\usepackage{pgfplots} % For plots
\pgfplotsset{compat=1.18} % Set compatibility level
\usepackage{tikz} % For drawing shapes
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem*{example}{Example}
\newtheorem{remark}{Remark}
\reversemarginpar
\begin{document}
%------- Title page   -----------
\title{MATH 325: Honours Ordinary Differential Equations}
\author{William Homier}
\affil[1]{McGill University Physics, 3600 Rue University, Montréal, QC H3A 2T8, Canada}
\date{January \(6^{th}\), 2026}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\maketitle

%------- Abstract -----------
\noindent\rule{\textwidth}{0.4pt}
\thispagestyle{empty}
\begin{abstract}

\end{abstract}
\noindent\rule{\textwidth}{0.4pt}
\clearpage

%------- Table of Contents -----------
\thispagestyle{empty}
\hypersetup{
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\tableofcontents
\clearpage

%------- introduction -----------
\setcounter{page}{1}
\section{Introduction}
Jean-Philippe Lessard (Burnside 1119).
Tutorials every wednesday from 9am to 10am, ENGTR 0070, with Eunpyo Bang.
Office hours thursday.
No textbooks.
25\% assignments (2 written assignments 15\%, and 5 webworks 10\%).
25\% Midterm (February 16 - inclass).
50\% Final.
Since its honours you will deal with analysis.

\section{Prerequisite knowledge}
\subsection{Analysis}

\section{Intro, Classification, Theorem of Existence \& Uniqueness}
\subsection{Intro}
\marginpar{January 06, 2026.}
\begin{definition}[Differential Equation]
    A differential equation (DE) is a relation that involves an unknown function and some of its derivatives.
\end{definition}
To better understand what a differential equation is, consider the following example.\\
Imagine a ball of mass \(m\) falling, subject to gravity and air resistance (drag). Denote by \(v(t)\) the velocity of the ball at time \(t\), whereas \(t\) is the independent variable, and \(v\) the dependent variable. Let the downward direction be positive. We know the force of gravity is given by \(F_g = mg\), where \(g\) is the acceleration due to gravity. The drag force is given by \(F_d = -\lambda v\), where \(\lambda\) is the drag coefficient and is \(\lambda \geq 0\). According to Newton's second law \(\sum F = ma\), the net force acting on the ball is equal to its mass times its acceleration
\[m\frac{dv}{dt} = mg - \lambda v.\]
Let \(y(t)\) be the position, meaning \(v(t) = \frac{dy}{dt}\). Then, we can rewrite the above equation as
\[my'' + \lambda y' = mg.\]

Let's analyze another example, population growth (known as the Malthusian growth model).\\
Denote by \(N(t)\) the size of a given population at time t. In an "unconstraint" environment, it is reasonable to assume that the rate of change of the number of individuals is proportional to the number of individuals present. This assumption leads to the following differential equation:
\[\frac{dN}{dt} = rN,\]
where \(r\) is called the growth rate (if \(r > 0\)), and decay rate (if \(r < 0\)). Assume that \(N > 0\). Using the chain rule and assuming that \(N(t)\) satisfies \(N' = rN\)
\[\frac{d}{dt}ln(N(t)) = \frac{dln(N)}{dN} \cdot \frac{dN}{dt} = \frac{1}{N} \cdot N' = r,\]
integrate with respect to t
\[ln(N(t)) = rt + C,\]
where \(C\) is the constant of integration. Exponentiating both sides, we obtain
\[N(t) = e^{ln(N(t))} = e^Ce^{rt} = ke^{rt},\]
where \(\{k > 0 | k \in \mathbb{R}\}\) which could be any positive constant is the initial population size at time \(t = 0\).\\
Assume that an initial population (condition) is given:
\[N(0) = N_0 (fixed),\]
we therefore get that \(k = N_0\), and the unique solution that satisfies the initial condition is
\[N(t) = N_0e^{rt}.\]

The problem with the answer we got in the previous example is that it is not realistic in the long run, how about we consider a carrying capacity\footnote{maximum population size that the environment can sustain indefinitely}. This leads us to another example: Population growth/decay with the carrying capacity of the environment.\\
Now assume that our growth rate depends on the population size \(N(t)\) itself, therefore we get that
\[\frac{dN}{dt} = R(N)N.\]
Denote by K the number of individual that the environment can carry. K is called the carrying capacity of the environment. If \(N < K\), we want growth \((R(N) > 0)\) and if \(N > K\), we want decay \((R(N) < 0)\).\\
\begin{center}
\begin{tikzpicture}[scale=1.1]
    % Axes
    \draw[->] (-0.5,0) -- (6,0) node[right] {$N$};
    \draw[->] (0,-0.5) -- (0,5) node[above] {$R(N)$};

    % Line
    \draw[thick] (0,4) -- (5,0);

    % Intercepts
    \fill (0,4) circle (2pt) node[left] {$r$};
    \fill (5,0) circle (2pt) node[below] {$K$};

\end{tikzpicture}
\end{center}
Let's pick the simplest function \(R(N)\) that satisfies \(R(0) = r, R(K) = 0\) and is linear. We get that
\[R(N) = r(1 - \frac{N}{K}).\]
Therefore, our differential equation becomes
\[\frac{dN}{dt} = r(1 - \frac{N}{K})N = \frac{r}{K}(K - N(t))N(t).\]
This is called the logistic equation.

\begin{definition}[Ordinary Differential Equation]
    An ordinary differential equation (ODE) is a differential equation whose unknown function depends on one independent variable only.
\end{definition}
Example of ODEs:
\begin{itemize}
    \item \(y''(t) + y'(t) + 2y(t) = sin(t)\)
    \item \(N'(t) = rN(t)\)
    \item \(mv'(t) = mg - \lambda v(t)\)
    \item \(y'(x) + 3y(x) = e^x\)
\end{itemize}
\begin{definition}[Partial Differential Equation]
    A partial differential equation (PDE) is a differential equation whose unknown function depends on more than one independent variable. \textbf{Will not be taught in this course.}
\end{definition}
Example of a PDE is the Heat Equation. Let \(u = u(x,t)\), \(\frac{\partial u}{\partial t} = \frac{\partial^2 u}{\partial x^2}\). This PDE denotes the temperature of a body at time t and at position x.

\subsection{Classification}
\marginpar{January 08, 2026.}
\subsubsection{The Order}
\begin{definition}
    The order of an ODE is the order of the highest derivative that appears in the equation.
\end{definition}
\begin{example}
    \(N' = rN\) (first order ODE)
\end{example}
\begin{example}
    \(y''(t) + 2y'(t) = e^t\) (second order ODE)
\end{example}
Given \(n \in \mathbb{N}\), an \(n\)\textsuperscript{th} order scalar ODE is written as
\[F(t, y(t), y'(t), y''(t), \ldots, y^{(n)}(t)) = 0,\]
where \(F : \mathbb{R}^{n + 2} \to \mathbb{R}\) is a map and where \(y^{(k)}(t) = \frac{d^ky}{dt^k}(t), k = 1, \ldots, n\).\\\\

\noindent\textbf{Systems of first order ODEs}\\
\indent Consider a map \(f: D \times (a,b) \to \mathbb{R}^n\), where \(D \subseteq \mathbb{R}^n\) is an open set, and (a,b) is a "time" interval. A general first order system of ODEs is given by \(y'(t) = f(y(t), t)\), where \(y(t) = \begin{pmatrix}y_1(t)\\ \vdots \\ y_n(t)\end{pmatrix}\), \(f = \begin{pmatrix}f_1\\ \vdots \\ f_n\end{pmatrix}\), and \(y' = \begin{pmatrix}y'_1\\ \vdots \\ y'_n\end{pmatrix}\).\\
\textbf{Remark:} Assume that a scalar \(n\)\textsuperscript{th} order ODE has the form \[y^{(n)}(t) = G(t, y(t), y'(t), \ldots, y^{(n-1)}(t)).\]
Letting \(y_1 = y, y_2 = y', \ldots, y_n = y^{(n-1)}\). This leads us to \(y_1' = y' = y_2, y_2' = y'' = y_3, \ldots, y_{n-1}' = y^{(n-1)} = y_n, y_n' = y^{(n)} = G(t, y_1, y_2, \ldots, y_n)\). Therefore, we can rewrite the \(n\)\textsuperscript{th} order ODE as a first order system of ODEs:
\[\begin{pmatrix}y_1'\\y_2'\\\vdots\\y_{n-1}'\\y_n'\end{pmatrix} = \begin{pmatrix}y_2\\y_3\\\vdots\\y_n\\G(t, y_1, y_2, \ldots, y_n)\end{pmatrix}\]

\begin{example}[Lorenz Equation]
    \[y_1' = \sigma(y_2 - y_1)\]
    \[y_2' = \rho y_1 - y_2 - y_1y_3\]
    \[y_3' = y_1y_2 - \beta y_3\]
    where \(\sigma, \rho, \beta\) are parameters. n = 3. This is a first order system of ODEs. They are nonlinear because of the products \(y_1y_3\) and \(y_1y_2\).
\end{example}

\subsubsection{Linearity}
\begin{definition}[Linearity]
    The n\textsuperscript{th} order ODE \(F(t, y, y', \ldots, y^{(n)}) = 0\) is linear if \(F\) is a linear polynomial in the variables \(y, y', y'', \ldots, y^{(n)}\), that is, it is of the form \(a_0(t)y^{(n)}(t) + a_1(t)y^{(n-1)}(t) + \ldots + a_{n-1}(t)y'(t) + a_n(t)y(t) = g(t)\), where \(a_0, a_1, \ldots, a_n, g\) are given functions of t. Otherwise, it is nonlinear.
\end{definition}
In short terms, an ODE is said to be linear if it can be written as \(y'(t) = A(t)y(t) + r(t)\) where, given \(t \in (a,b)\), \(A(t) \in M_n(\mathbb{R})\) (the set of \(n \times n\) real matrices) and \(r(t) \in \mathbb{R}^n\).

\begin{example}
Consider the second-order ODE
\[y'' + 2y' + y = e^t\]
\[\Longrightarrow y'' = -2y' - y + e^t.\]

Define new variables
\[y_1 = y, \qquad y_2 = y'.\]

Then the system becomes
\[\begin{pmatrix} y_1' \\ y_2' \end{pmatrix}=\begin{pmatrix} 0 & 1 \\ -1 & -2 \end{pmatrix}\begin{pmatrix} y_1 \\ y_2 \end{pmatrix}
+\begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]

Let
\[A = \begin{pmatrix} 0 & 1 \\ -1 & -2 \end{pmatrix},\qquad r(t) = \begin{pmatrix} 0 \\ e^t \end{pmatrix}.\]
\end{example}

\subsubsection{Autonomy}
\begin{definition}[Autonomy]
    The \(n\)\textsuperscript{th} order ODE \(F(t, y, y', \ldots, y^{(n)}) = 0\) is autonomous if \(F\) does not depend explicitly on t, that is, if it is of the form \(F(y, y', \ldots, y^{(n)}) = 0\). Otherwise, it is non-autonomous.
\end{definition}
\begin{example}
    \(y'' + 2y' + y - e^t = 0\) is non-autonomous.
\end{example}
\begin{example}
    \(N'(t) = rN(t)\) is autonomous.
\end{example}
\begin{example}
    \(y'(t) = ty(t)\) is non-autonomous.
\end{example}

\noindent Equivalently, a first-order system \(y' = f(y,t)\) is autonomous if it can be written as \[ y' = f(y). \] Otherwise, it is non-autonomous.\\

\textbf{Note:} The Lorenz system is an example of an autonomous system\footnote{Here “system” means the unknown \(y\) is vector-valued, e.g. \(y \in \mathbb{R}^m\), rather than scalar.}.

\subsubsection{Solutions of ODEs}
\begin{definition}[Solutions of ODEs]
    Let \(f: D \times (a,b) \to \mathbb{R}^n\). A solution of \(y'(t) = f(y(t), t)\) on an interval \(J \subset \mathbb{R}\) is a differentiable function \(y: J \to D \subset \mathbb{R}^n\), such that \(y'(t) = f(y(t), t), \forall t \in J\). \(t\) is the independent variable, and \(y = (y_1, \ldots, y_n)\) is the dependent variable.
\end{definition}

\noindent\textbf{Explicit Solutions}
\begin{example}
    Consider the ODE
    \[y' + y = 1.\]
    We can verify that \(y(t) = e^{-t} + 1\), and therefore \(y'(t) = -e^{-t}\), is a solution on \(\mathbb{R}\). Indeed,
    \[y' + y = -e^{-t} + (e^{-t} + 1) = 1.\]
    In this example, \(y = y(t)\) is explicitly given as a function of t (independent variable).
\end{example}

\noindent\textbf{Implicit solutions}
\begin{example}
Consider the ODE
\[y \frac{dy}{dx} = x.\]
This is a nonautonomous, nonlinear, first-order scalar ODE. Separating variables gives
\[y\,dy = x\,dx.\]
Integrating,
\[\frac{1}{2}y^2 = \frac{1}{2}x^2 + C,\]
or equivalently, the implicit solution
\[x^2 - y^2 = C, \qquad C \in \mathbb{R}.\]
To verify, differentiate implicitly:
\[\frac{d}{dx}(x^2 - y^2) = 0 \;\Longrightarrow\; 2x - 2y\frac{dy}{dx} = 0\;\Longrightarrow\; y\frac{dy}{dx} = x.\]
\end{example}

\marginpar{January 13, 2026}
\subsection{Initial Value Problems}

A first-order system of ODEs is written as
\[
y' = f(y,t), \quad f : D \times (a,b) \to \mathbb{R}^n.
\]
Let \(t_0 \in (a,b)\). An initial condition is
\[
y(t_0) = y_0 \in \mathbb{R}^n.
\]
An initial value problem (IVP) is
\[
\begin{cases}
y' = f(y,t),\\
y(t_0) = y_0.
\end{cases}
\]

\subsection{Existence and Uniqueness Theorem}

\subsubsection{Lipschitz continuity}

\begin{definition}[Lipschitz continuity]
Let \(D \subseteq \mathbb{R}^n\) and let \(||\cdot||\) be a norm on \(\mathbb{R}^n\).
A function \(f : D \to \mathbb{R}^n\) is Lipschitz continuous if there exists \(L \ge 0\) such that
\[
||f(y_1) - f(y_2)|| \le L||y_1 - y_2|| \quad \forall y_1,y_2 \in D.
\]
The smallest such \(L\) is called the Lipschitz constant and is denoted \(Lip(f)\).
\end{definition}

\begin{example}
Let \(f(y) = 4y - 5\), \(D = \mathbb{R}\), and \(||\cdot|| = |\cdot|\).
Then
\[
|f(y_1) - f(y_2)| = |4y_1 - 4y_2| = 4|y_1 - y_2|.
\]
So \(f\) is Lipschitz with \(Lip(f) = 4\).
\end{example}

\begin{example}
Let
\[
f(y) = \frac{1}{y - 1}, \quad D = (1, +\infty).
\]
Then \(f\) is not Lipschitz on \(D\).

Now fix \(\delta > 1\) and define \(D_\delta = (\delta, +\infty)\).
For \(y_1,y_2 \in D_\delta\), by the Mean Value Theorem, there exists \(z \in (y_1,y_2)\) such that
\[
f(y_2) - f(y_1) = f'(z)(y_2 - y_1),
\quad f'(y) = -\frac{1}{(y-1)^2}.
\]
So
\[
|f(y_2) - f(y_1)| \le \frac{1}{(z-1)^2}|y_2 - y_1|
\le \frac{1}{(\delta-1)^2}|y_2 - y_1|.
\]
Thus \(f\) is Lipschitz on \(D_\delta\) with
\[
Lip(f) = \frac{1}{(\delta - 1)^2}.
\]
\end{example}

\subsubsection{Local Lipschitz continuity}

\begin{definition}[Locally Lipschitz]
Let \(D \subseteq \mathbb{R}^n\) be open. A function \(f : D \to \mathbb{R}^n\) is locally Lipschitz if for every compact set \(K \subset D\), there exists \(L(K)\) such that
\[
||f(y_1) - f(y_2)|| \le L(K)||y_1 - y_2|| \quad \forall y_1,y_2 \in K.
\]
\end{definition}

\begin{problem}
(See tutorial 1)  
If \(f \in C^1(D)\), then \(f\) is locally Lipschitz.
\end{problem}

\subsubsection{Existence and Uniqueness Theorem}

\begin{theorem}[Existence and Uniqueness]
Let \(D \subseteq \mathbb{R}^n\) be open and let \((a,b)\) be an open interval containing \(t_0\).
Consider the IVP
\[
\begin{cases}
y' = f(y,t),\\
y(t_0) = y_0.
\end{cases}
\]
Assume \(f : D \times (a,b) \to \mathbb{R}^n\) is continuous and locally Lipschitz in \(y\).

If \(y_0 \in D\), then there exists an open interval \(J\) containing \(t_0\) on which a solution exists.
Moreover, this solution is unique on \(J\).
\end{theorem}

\subsubsection{Integral form of solutions}

\begin{lemma}
A function \(y\) solves the IVP if and only if
\[
y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)\,ds.
\]
\end{lemma}

\begin{proof}
If \(y' = f(y,t)\) and \(y(t_0) = y_0\), then by the Fundamental Theorem of Calculus,
\[
y(t) - y(t_0) = \int_{t_0}^{t} f(y(s),s)\,ds,
\]
so
\[
y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)\,ds.
\]

Conversely, differentiating the right-hand side gives
\[
y'(t) = f(y(t),t), \quad y(t_0) = y_0.
\]
\end{proof}

\subsubsection{Picard operator}

Let \((y_0,t_0) \in D \times (a,b)\). Since this set is open, there exist \(\alpha,\delta > 0\) such that
\[
D_{\alpha,\delta} = \{(y,t) : ||y - y_0|| \le \alpha, |t - t_0| \le \delta\}
\subset D \times (a,b).
\]
Define
\[M_{\alpha,\delta} = \sup_{(y,t) \in D_{\alpha,\delta}}\footnote{\(sup\) means the largest value in a set of numbers.} ||f(y,t)|| < +\infty.\]
Let
\[\epsilon = \min\left(\delta, \frac{\alpha}{M_{\alpha,\delta}}\right), \quad J = (t_0 - \epsilon, t_0 + \epsilon).\]

\begin{lemma}[Picard operator]
For any function \(y\) such that \(y(t_0)=y_0\) and \((y(t),t) \in D_{\alpha,\delta}\) for all \(t \in J\), define
\[
T(y)(t) = y_0 + \int_{t_0}^{t} f(y(s),s)\,ds.
\]
Then \(T(y)(t_0)=y_0\) and \((T(y)(t),t) \in D_{\alpha,\delta}\) for all \(t \in J\).
\end{lemma}

\begin{proof}
\[
T(y)(t_0) = y_0.
\]
For \(t \in J\),
\[
||T(y)(t) - y_0|| \le \int_{t_0}^{t} ||f(y(s),s)||ds
\le M_{\alpha,\delta}|t - t_0|
\le M_{\alpha,\delta}\epsilon \le \alpha.
\]
So \((T(y)(t),t) \in D_{\alpha,\delta}\).
\end{proof}

\marginpar{January 15, 2026}
\begin{lemma}
    If $y : J \to \mathbb{R}^n$ satisfies 
    \begin{enumerate}
        \item $y(t_0) = y_0$,
        \item $(y(t),t) \in D_{\alpha,\delta}$ for all $t \in J$,
    \end{enumerate}
    then $T(y) : J \to \mathbb{R}^n$ satisfies the same properties.
\end{lemma}
\paragraph{Picard iterations}
Define $y_0(t) = y_0$ (constant function) clearly satisfies (1) and (2).
For $k \geq 1$, define the Picard iterations by
\[y_k(t) = T(y_{k-1}(t)) = y_0 + \int_{t_0}^{t} f(y_{k-1}(s),s)ds.\]

\paragraph{Existance}
The Picard iterations converge uniformly to a function $y : J \to \mathbb{R}^n$ which satisfies (1) and (2), and is a solution of the IVP.

\begin{proof}
    Pick $t \in [t_0, t_0 + \epsilon]$ (the proof is similar for $t \in [t_0 - \epsilon, t_0]$).
    The goal is to show first that $\{y_k(t)\}_{k=0}^{\infty}$ is a Cauchy sequence\footnote{Cauchy sequence is a sequence that has a limit in a metric space $\mathbb{R}^n$.} in $\mathbb{R}^n$. We prove by induction that
    \[(**) : ||y_m(t) - y_{m - 1}(t)|| \leq L^{m - 1}M_{\alpha,\delta} \frac{(t - t_0)^m}{m!},\:\forall m \geq 1.\]
    For $m = 1$,
    \[||y_1(t) - y_0(t)|| = ||y_0 + \int_{t_0}^{t} f(y_0,s)ds - y_0|| \leq \int_{t_0}^{t}||f(y_0,s)||ds \leq M_{\alpha,\delta}|t - t_0| \leq \alpha.\]

    Assume $(**)$ holds for $m$ and show that it holds for $m + 1$
    \[||y_{m + 1}(t) - y_m(t)|| = ||y_0 + \int_{t_0}^{t}f(y_m(s),s)ds - y_0 - \int_{t_0}^{t}f(y_{m-1}(s),s)ds|| \leq \int_{t_0}^{t}||f(y_m(s),s) - f(y_{m-1}(s),s)||ds.\]

    Since $D_{\alpha,\delta}$ is compact and $f$ is (LLC) Lipschitz on $D_{\alpha,\delta}$ with Lipschitz constant $L$. Therefore, there exists $L = L(D_{\alpha,\delta})$ such that
    \[||f(x,t) - f(y,t)|| \leq L||x - y||,\:\forall (x,t),(y,t) \in D_{\alpha,\delta}.\]
    Which implies
    \[\leq \int_{t_0}^{t}||y_m(s) - y_{m - 1}(s)||ds. \leq\footnote{By the (**)} L\int_{t_0}^{t}L^{m - 1}M_{\alpha,\delta} \frac{(t - t_0)^{m - 1}}{(m - 1)!}ds \leq L^{m - 1}M_{\alpha,\delta} \frac{(s - t_0)^m}{m!}ds\]
    \[= L^m \frac{M_{\alpha,\delta}}{m!} \int_{t_0}^{t}(s - t_0)^mds = L^m M_{\alpha,\delta}\frac{(t - t_0)^m}{(m+1)!}.\]
    In particular, for all $\rho \geq 1$,
    \[||y_\rho(t) - y_{\rho-1}(t)|| \leq M_{\alpha,\delta} \frac{(L(t - t_0))^{\rho}}{(\rho)!} < \frac{M_{\alpha,\delta}}{L}\frac{(L\epsilon)^\rho}{\rho!}.\]
    Pick $m,p \geq 1$ be two fixed integers
    \[||y_{m+p}(t) - y_{m+1}(t)|| = ||y_{m+p} - y_{m+p-1} + y_{m+p-1} + \ldots - y_{m+2} + y_{m+2} - y_{m+1}||.\]

    Triangle inequality gives
    \[\leq \sum_{k_1}^{p-1}||y_{m+k+1}(t) - y_{m+k}(t)||\]
    \[< \sum_{k=1}^{p-1}\frac{(L\epsilon)^{m+k+1}}{(m+k+1)!} =\footnote{(***)} \frac{M_{\alpha,\delta}}{L}\sum_{j=m+2}^{m+p}\frac{(L\epsilon)^j}{j!}.\]
    Where $j = m+k+1$. Now
    \[\to_{m,p \to +\inf} 0\]
    since $e^{L\epsilon} = \sum_{j=0}^{+\inf}\frac{(L\epsilon)^j}{j!}$ converges.
    \(\to \{y_k(t)\}_{k=0}^{\infty}\) is a Cauchy sequence in \(\mathbb{R}^n\). Since $\mathbb{R}^n$ is complete (all cauchy sequences converge) $y_k(t)$ converges to a limit, that we denote $y(t)$.
    Take the limit when $p \to +\inf$ in (***) :
    \[y(t) - y_{m+1}(t) \leq \frac{M_{\alpha,\delta}}{L}\sum_{j=m+2}^{+\inf}\frac{(L\epsilon)^j}{j!} \to_{m \to +\inf} 0.\]
    This implies that $y_k(t)$ converges uniformly to $y(t)$. By construction, $y(t_0) = y_0$ is continuous, $y_1,y_2,y_3,\ldots$ are also continuous.
    By the uniform convergence theorem, $y(t)$ is continuous.
    By Picard's iterations,
    \[y_k(t) = y_0 + \int_{t_0}^{t} f(y_{k-1}(s),s)ds.\]
    Taking the limit when \(k \to +\infty\)
    \[y(t) = y_0 + \lim_{k \to +\infty} \int_{t_0}^{t} f(y_{k-1}(s),s)ds =\footnote{Uniform convergence} y_0 + \int_{t_0}^{t}\lim_{k \to +\infty} f(y_{k-1}(s),s)ds = y_0 + \int_{t_0}^{t} f(y(s),s)ds.\]
\end{proof}\footnote{Evaluated on compacted cylinder, understanding epsilon, the L lipschitz constant coming from somehwere, not on the analysis background such as...}
\begin{proof}[Uniqueness]
    Assume that $y(t)$ and $z(t)$ satisfy the IVP
    \[\begin{cases}y' = f(y,t),\\ y(t_0) = y_0.\end{cases}\]
    Which implies that
    \[\begin{cases}
        y(t) = y_0 + \int_{t_0}^{t} f(y(s),s)ds,\\
        z(t) = y_0 + \int_{t_0}^{t} f(z(s),s)ds.
    \end{cases}\]
    \[||y(t) - z(t)|| \leq \int_{t_0}^{t}||f(y(s),s) - f(z(s),s)||ds \leq \int_{t_0}^{t}||y(s) - z(s)||ds.\]
    Define \(g(t) = \int_{t_0}^{t}||y(s) - z(s)||ds\). Therefore,
    \[g'(t) = ||y(t) - z(t)|| \leq Lg(t).\]
    Which implies that
    \[g'(t) - Lg(t) \leq 0.\]
    Multiply both sides by \(e^{-L(t-t_0)}\)
    \[\frac{d}{dt}(e^{-L(t-t_0)}g(t)) = (g'(t) - Lg(t))e^{-L(t-t_0)} \leq 0.\]
    Which implies that \(e^{-L(t-t_0)}g(t)\) is decreasing. Therefore, for all $t \geq t_0, \quad e^{-L(t-t_0)}g(t) \leq e^{-L(t-t_0)}g(t_0)$
\end{proof}
\section{}
\section{}
\section{}
\section{}
\section{}


\section{Solutions}

\section{Appendix}

\section{Useful Links}

\end{document}